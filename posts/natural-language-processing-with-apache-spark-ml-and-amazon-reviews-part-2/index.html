<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content=True><meta name=MobileOptimized content=320><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content=no-referrer><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400" rel=stylesheet type=text/css><link rel=icon type=image/png href=https://reorchestrate.com/favicon_16x16.png sizes=16x16><link rel=icon type=image/png href=https://reorchestrate.com/favicon_32x32.png sizes=32x32><link rel=icon type=image/png href=https://reorchestrate.com/favicon_128x128.png sizes=128x128><title>Natural Language Processing with Apache Spark ML and Amazon Reviews (Part 2)</title><link rel=canonical href=https://reorchestrate.com/posts/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-2/><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;-webkit-font-smoothing:antialiased;line-height:160%;color:#1d1313;max-width:950px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:#0000;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3,h4,h5{font-family:old standard tt,serif}strong{font-weight:600}table{width:100%}thead{font-weight:800;background:#dadada}tbody tr:nth-child(even){background:#f7f7f7}table td,th{padding:2px}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-weight:700;font-size:35px}h2{font-weight:700;font-size:28px}h3{font-weight:700;font-size:22px;margin-top:28px}h4{font-weight:700;font-size:20px;margin-top:20px}h5{font-size:18px;margin-top:18px}h1 a,h2 a,h3 a,h4 a,h5 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,time{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content time{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#source{margin:50px 0 0}#links{margin:50px 0 0}#links :nth-child(2){float:right}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}#nav .sub{font-size:80%;display:inline-block;vertical-align:middle}#nav .sub p{color:#403c3b;margin:0}#nav .sub a{text-decoration:none}#nav .sub svg{fill:#403c3b;display:inline-block;vertical-align:middle;padding-bottom:3px;padding-right:5px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}.chroma{background-color:#f0f0f0}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em}.chroma .ln{margin-right:.4em;padding:0 .4em}.chroma .k{color:#007020;font-weight:700}.chroma .kc{color:#007020;font-weight:700}.chroma .kd{color:#007020;font-weight:700}.chroma .kn{color:#007020;font-weight:700}.chroma .kp{color:#007020}.chroma .kr{color:#007020;font-weight:700}.chroma .kt{color:#902000}.chroma .na{color:#4070a0}.chroma .nb{color:#007020}.chroma .nc{color:#0e84b5;font-weight:700}.chroma .no{color:#60add5}.chroma .nd{color:#555;font-weight:700}.chroma .ni{color:#d55537;font-weight:700}.chroma .ne{color:#007020}.chroma .nf{color:#06287e}.chroma .nl{color:#002070;font-weight:700}.chroma .nn{color:#0e84b5;font-weight:700}.chroma .nt{color:#062873;font-weight:700}.chroma .nv{color:#bb60d5}.chroma .s{color:#4070a0}.chroma .sa{color:#4070a0}.chroma .sb{color:#4070a0}.chroma .sc{color:#4070a0}.chroma .dl{color:#4070a0}.chroma .sd{color:#4070a0;font-style:italic}.chroma .s2{color:#4070a0}.chroma .se{color:#4070a0;font-weight:700}.chroma .sh{color:#4070a0}.chroma .si{color:#70a0d0;font-style:italic}.chroma .sx{color:#c65d09}.chroma .sr{color:#235388}.chroma .s1{color:#4070a0}.chroma .ss{color:#517918}.chroma .m{color:#40a070}.chroma .mb{color:#40a070}.chroma .mf{color:#40a070}.chroma .mh{color:#40a070}.chroma .mi{color:#40a070}.chroma .il{color:#40a070}.chroma .mo{color:#40a070}.chroma .o{color:#666}.chroma .ow{color:#007020;font-weight:700}.chroma .c{color:#60a0b0;font-style:italic}.chroma .ch{color:#60a0b0;font-style:italic}.chroma .cm{color:#60a0b0;font-style:italic}.chroma .c1{color:#60a0b0;font-style:italic}.chroma .cs{color:#60a0b0;background-color:#fff0f0}.chroma .cp{color:#007020}.chroma .cpf{color:#007020}.chroma .gd{color:#a00000}.chroma .ge{font-style:italic}.chroma .gr{color:red}.chroma .gh{color:navy;font-weight:700}.chroma .gi{color:#00a000}.chroma .go{color:#888}.chroma .gp{color:#c65d09;font-weight:700}.chroma .gs{font-weight:700}.chroma .gu{color:purple;font-weight:700}.chroma .gt{color:#04d}.chroma .w{color:#bbb}</style></head><body><section id=nav><h1><a href=https://reorchestrate.com/>reorchestrate</a></h1><section class=section><div class=sub><p>&copy; Mike Seddon 2024 |
<a href=https://www.github.com/seddonm1 target=_blank><svg width="22" height="22" viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M896 128q209 0 385.5 103T1561 510.5 1664 896q0 251-146.5 451.5T1139 1625q-27 5-40-7t-13-30q0-3 .5-76.5t.5-134.5q0-97-52-142 57-6 102.5-18t94-39 81-66.5 53-105T1386 856q0-119-79-206 37-91-8-204-28-9-81 11t-92 44l-38 24q-93-26-192-26t-192 26q-16-11-42.5-27T578 459.5 493 446q-45 113-8 204-79 87-79 206 0 85 20.5 150t52.5 105 80.5 67 94 39 102.5 18q-39 36-49 103-21 10-45 15t-57 5-65.5-21.5T484 1274q-19-32-48.5-52t-49.5-24l-20-3q-21 0-29 4.5t-5 11.5 9 14 13 12l7 5q22 10 43.5 38t31.5 51l10 23q13 38 44 61.5t67 30 69.5 7 55.5-3.5l23-4q0 38 .5 88.5t.5 54.5q0 18-13 30t-40 7q-232-77-378.5-277.5T128 896q0-209 103-385.5T510.5 231 896 128zM419 1231q3-7-7-12-10-3-13 2-3 7 7 12 9 6 13-2zm31 34q7-5-2-16-10-9-16-3-7 5 2 16 10 10 16 3zm30 45q9-7 0-19-8-13-17-6-9 5 0 18t17 7zm42 42q8-8-4-19-12-12-20-3-9 8 4 19 12 12 20 3zm57 25q3-11-13-16-15-4-19 7t13 15q15 6 19-6zm63 5q0-13-17-11-16 0-16 11 0 13 17 11 16 0 16-11zm58-10q-2-11-18-9-16 3-14 15t18 8 14-14z"/></svg></a><a href=https://www.linkedin.com/in/mikeseddonau/ target=_blank><svg width="22" height="22" viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M365 1414h231V720H365v694zm246-908q-1-52-36-86t-93-34-94.5 34-36.5 86q0 51 35.5 85.5T479 626h1q59 0 95-34.5t36-85.5zm585 908h231v-398q0-154-73-233t-193-79q-136 0-209 117h2V720H723q3 66 0 694h231v-388q0-38 7-56 15-35 45-59.5t74-24.5q116 0 116 157v371zm468-998v960q0 119-84.5 203.5T1376 1664H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z"/></svg></a></p></div></section><ul></ul></section><section id=content><h1>Natural Language Processing with Apache Spark ML and Amazon Reviews (Part 2)</h1><div id=sub-header>6 December 2015</div><div class=entry-content><p>Continues from <a href=https://reorchestrate.com/posts/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/>Part 1</a>.</p><h4 id=4-execution>4 Execution</h4><h5 id=4-1-the-pipeline>4.1 The Pipeline</h5><p>Now we have all the components of the pipeline ready all that is needed is to load them into the Spark ML <code>Pipeline()</code>. A pipeline helps with the sequencing of stages so that we can automate the pipeline in the image at the top of this post.</p><p>When <code>Pipeline()</code> is <code>fit()</code> to the <code>training</code> set it will call the <code>fit()</code> method of the two estimator stages (<code>StringIndexer()</code> and <code>NaiveBayes()</code> which will both produce models <code>StringIndexerModel()</code> and <code>NaiveBayesModel()</code> respectively) and <code>transform()</code> on all the rest of the stages. When the <code>Pipeline()</code> is called to <code>transform()</code> the <code>test</code> set it will call <code>transform()</code> on all the stages (including the models).</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>pipeline</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>Pipeline</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setStages</span><span class=o>(</span><span class=nc>Array</span><span class=o>(</span><span class=n>regexTokenizer</span><span class=o>,</span> <span class=n>remover</span><span class=o>,</span> <span class=n>stemmer</span><span class=o>,</span> <span class=n>ngram2</span><span class=o>,</span> <span class=n>ngram3</span><span class=o>,</span> <span class=n>removerHashingTF</span><span class=o>,</span> <span class=n>ngram2HashingTF</span><span class=o>,</span> <span class=n>ngram3HashingTF</span><span class=o>,</span> <span class=n>assembler</span><span class=o>,</span> <span class=n>labelIndexer</span><span class=o>,</span> <span class=n>nb</span><span class=o>,</span> <span class=n>labelConverter</span><span class=o>))</span>
<span class=o>}</span></code></pre></div><h5 id=4-2-pipeline-stage-parameters>4.2 Pipeline Stage Parameters</h5><p>Many of the stages in the pipeline take parameters, for example a key parameter in for the <code>HashingTF</code> transformer is <code>.numFeatures(n)</code>. As we have multiple stages with potentially many parameters, and we don&rsquo;t what the best parameters would be, what would be ideal is if we could test different parameters sets iteratively and see which set produces the best model.</p><p>The <code>ParamGridBuilder()</code> object allows us to easily <code>build()</code> an array of all the possible combinations of parameters we want to test:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=c1>// We use a ParamGridBuilder to construct a grid of parameters to search over.
</span><span class=c1>// With 3 stages of hashingTF.numFeatures each with 2 values (1000,10000)
</span><span class=c1>// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.
</span><span class=c1></span><span class=k>val</span> <span class=n>paramGrid</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>ParamGridBuilder</span><span class=o>()</span>
  <span class=o>.</span><span class=n>addGrid</span><span class=o>(</span><span class=n>removerHashingTF</span><span class=o>.</span><span class=n>numFeatures</span><span class=o>,</span> <span class=nc>Array</span><span class=o>(</span><span class=mi>1000</span><span class=o>,</span><span class=mi>10000</span><span class=o>))</span>
  <span class=o>.</span><span class=n>addGrid</span><span class=o>(</span><span class=n>ngram2HashingTF</span><span class=o>.</span><span class=n>numFeatures</span><span class=o>,</span> <span class=nc>Array</span><span class=o>(</span><span class=mi>1000</span><span class=o>,</span><span class=mi>10000</span><span class=o>))</span>
  <span class=o>.</span><span class=n>addGrid</span><span class=o>(</span><span class=n>ngram3HashingTF</span><span class=o>.</span><span class=n>numFeatures</span><span class=o>,</span> <span class=nc>Array</span><span class=o>(</span><span class=mi>1000</span><span class=o>,</span><span class=mi>10000</span><span class=o>))</span>
  <span class=o>.</span><span class=n>build</span><span class=o>()</span>
<span class=o>}</span></code></pre></div><p>Output:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Array</span><span class=o>({</span>
        <span class=n>hashingTF_2b4181ee161b</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span><span class=o>,</span>
        <span class=n>hashingTF_410c0a05ff35</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span><span class=o>,</span>
        <span class=n>hashingTF_e63a89fd73ba</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span>
<span class=o>},</span> <span class=o>{</span>
        <span class=n>hashingTF_2b4181ee161b</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>10000</span><span class=o>,</span>
        <span class=n>hashingTF_410c0a05ff35</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span><span class=o>,</span>
        <span class=n>hashingTF_e63a89fd73ba</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span>
<span class=o>},</span> <span class=o>{</span>
        <span class=n>hashingTF_2b4181ee161b</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span><span class=o>,</span>
        <span class=n>hashingTF_410c0a05ff35</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>10000</span><span class=o>,</span>
        <span class=n>hashingTF_e63a89fd73ba</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span>
<span class=o>},</span> <span class=o>{</span>
        <span class=n>hashingTF_2b4181ee161b</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>10000</span><span class=o>,</span>
        <span class=n>hashingTF_410c0a05ff35</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>10000</span><span class=o>,</span>
        <span class=n>hashingTF_e63a89fd73ba</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span>
<span class=o>},</span> <span class=o>{</span>
        <span class=n>hashingTF_2b4181ee161b</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span><span class=o>,</span>
        <span class=n>hashingTF_410c0a05ff35</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>1000</span><span class=o>,</span>
        <span class=n>hashingTF_e63a89fd73ba</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>10000</span>
<span class=o>},</span> <span class=o>{</span>
        <span class=n>hashingTF_2b4181ee161b</span><span class=o>-</span><span class=n>numFeatures</span><span class=k>:</span> <span class=err>10000</span><span class=o>,</span>
        <span class=n>hashingTF_410c0a05</span><span class=o>...</span></code></pre></div><p>Now we need to pass our <code>Pipeline()</code> and the <code>ParamGridBuilder()</code> object to a <code>CrossValidator()</code>. The CrossValidator will execute the <code>pipeline</code> with each of the <code>paramGrid</code> parameters sets and test them using a <code>MulticlassClassificationEvaluator()</code> to determine the best model.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>cv</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>CrossValidator</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setEstimator</span><span class=o>(</span><span class=n>pipeline</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setEvaluator</span><span class=o>(</span><span class=k>new</span> <span class=nc>MulticlassClassificationEvaluator</span><span class=o>().</span><span class=n>setLabelCol</span><span class=o>(</span><span class=s>&#34;indexedLabel&#34;</span><span class=o>).</span><span class=n>setPredictionCol</span><span class=o>(</span><span class=s>&#34;prediction&#34;</span><span class=o>).</span><span class=n>setMetricName</span><span class=o>(</span><span class=s>&#34;precision&#34;</span><span class=o>))</span>
  <span class=o>.</span><span class=n>setEstimatorParamMaps</span><span class=o>(</span><span class=n>paramGrid</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setNumFolds</span><span class=o>(</span><span class=mi>2</span><span class=o>)</span> <span class=c1>// Use 3+ in practice
</span><span class=c1></span><span class=o>}</span></code></pre></div><p>To execute the model we call the <code>CrossValidator().fit()</code> method. After trying all combinations and comparing their results against a test set it will determine which is the best model. It is possible to get the model by calling <code>cvModel.bestModel</code> which then could be saved for future use.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>cvModel</span> <span class=k>=</span> <span class=n>cv</span><span class=o>.</span><span class=n>fit</span><span class=o>(</span><span class=n>training</span><span class=o>)</span></code></pre></div><p>Once we have the model we are able to use it to transform a dataset which will append a column of predictions based on the parameter set in <code>NaiveBayes().setPredictionCol()</code>.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>predictions</span> <span class=k>=</span> <span class=n>cvModel</span><span class=o>.</span><span class=n>transform</span><span class=o>(</span><span class=n>test</span><span class=o>)</span></code></pre></div><h4 id=5-evaluating-a-model>5 Evaluating a Model</h4><p>While it is very easy to write code to compare the predicted label to the input label of a dataset Spark ML provides a set of model Evaluators which can do the job for you:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=c1>// Select (prediction, true label) and compute test error
</span><span class=c1></span><span class=k>val</span> <span class=n>evaluator</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>MulticlassClassificationEvaluator</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setLabelCol</span><span class=o>(</span><span class=s>&#34;indexedLabel&#34;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setPredictionCol</span><span class=o>(</span><span class=s>&#34;prediction&#34;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setMetricName</span><span class=o>(</span><span class=s>&#34;precision&#34;</span><span class=o>)</span>
<span class=o>}</span>
<span class=k>val</span> <span class=n>precision</span> <span class=k>=</span> <span class=n>evaluator</span><span class=o>.</span><span class=n>evaluate</span><span class=o>(</span><span class=n>predictions</span><span class=o>)</span>
<span class=n>println</span><span class=o>(</span><span class=s>&#34;Test Error = &#34;</span> <span class=o>+</span> <span class=o>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>precision</span><span class=o>))</span></code></pre></div><p>If everything has run correctly you should see a result something like this:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Precision</span><span class=k>:</span> <span class=kt>Double</span> <span class=o>=</span> <span class=mf>0.44108</span>
<span class=nc>Test</span> <span class=nc>Error</span> <span class=k>=</span> <span class=mf>0.55892</span>
<span class=nc>Duration</span><span class=k>:</span> <span class=err>1893607</span> <span class=kt>ms</span> <span class=o>(</span><span class=kt>~</span><span class=err>31</span> <span class=kt>minutes</span><span class=o>)</span></code></pre></div><p>That is only 44.1%! meaning if we were to use this model more than half my predictions would be incorrect. Here is a good explanation why:</p><p><img src=https://reorchestrate.com/img/2015/12/ErrorModelComplexity.png alt="Error vs. Data Size"></p><p>The problem is that we are trying to predict a very large and complex problem with a very small dataset of 20000 reviews for each label. We are running a low complexity model (due to small sample size) of a training sample which, based on the blue line, indicates that we will have a high rate of prediction error - which we observed with a 55.89% error rate.</p><h5 id=5-1-increase-the-training-set>5.1 Increase the training set</h5><p>Once upon a time processing 100000 reviews would have been incredibly difficult due to lack of compute but with scalable architectures like Spark we should be able to process a much larger dataset. So let&rsquo;s run the same code but increase the size of the input dataset to be one 10 times larger so we have 200000 reviews for each label. This should result in a more complex model and hopefully reduced prediction error:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Precision</span><span class=k>:</span> <span class=kt>Double</span> <span class=o>=</span> <span class=mf>0.468264</span>
<span class=nc>Test</span> <span class=nc>Error</span> <span class=k>=</span> <span class=mf>0.531736</span>
<span class=nc>Duration</span><span class=k>:</span> <span class=err>11918777</span> <span class=kt>ms</span> <span class=o>(</span><span class=kt>~</span><span class=err>198</span> <span class=kt>minutes</span><span class=o>)</span></code></pre></div><p>So now we have gone from 44.1% to 46.8% accuracy and the time blew out from 31 mins to 198 mins (+538%). So we can see the effects of increasing sample size does roughly follow the blue line but at quite a large cost. Ideally I would like to run a much larger dataset through but I don&rsquo;t have the resources. I am adding more CPU/RAM soon so that would hopefully allow larger training sets and hopefully a better model.</p><h5 id=5-2-reduce-the-labels>5.2 Reduce the labels</h5><p>Another option would be to reduce the possible labels the model has to distinguish between whilst keeping the input dataset static.</p><p>We can do that in two ways, we either change the output options by trying to predict only <code>1</code> and <code>5</code> labels:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>var</span> <span class=n>reviewsDF</span> <span class=k>=</span> <span class=n>sqlContext</span><span class=o>.</span><span class=n>sql</span><span class=o>(</span>
<span class=s>s&#34;&#34;&#34;
</span><span class=s>  SELECT text, label, rowNumber FROM (
</span><span class=s>    SELECT
</span><span class=s>       reviews.overall AS label
</span><span class=s>      ,reviews.reviewText AS text
</span><span class=s>      ,row_number() OVER (PARTITION BY overall ORDER BY rand()) AS rowNumber
</span><span class=s>    FROM reviews
</span><span class=s>    WHERE reviews.overall IN (1,5)
</span><span class=s>  ) reviews
</span><span class=s>  WHERE rowNumber &lt;= </span><span class=si>$datasize</span><span class=s>
</span><span class=s>  &#34;&#34;&#34;</span>
<span class=o>)</span>

<span class=o>+-----+-----+</span>
<span class=o>|</span><span class=n>label</span><span class=o>|</span><span class=n>count</span><span class=o>|</span>
<span class=o>+-----+-----+</span>
<span class=o>|</span>  <span class=mf>1.0</span><span class=o>|</span><span class=mi>20000</span><span class=o>|</span>
<span class=o>|</span>  <span class=mf>5.0</span><span class=o>|</span><span class=mi>20000</span><span class=o>|</span>
<span class=o>+-----+-----+</span></code></pre></div><p>or we can compress our label options by grouping certain labels:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>var</span> <span class=n>reviewsDF</span> <span class=k>=</span> <span class=n>sqlContext</span><span class=o>.</span><span class=n>sql</span><span class=o>(</span>
<span class=s>s&#34;&#34;&#34;
</span><span class=s>  SELECT text, label, rowNumber FROM (
</span><span class=s>    SELECT
</span><span class=s>       CASE
</span><span class=s>              WHEN reviews.overall  = 1 THEN 1
</span><span class=s>              WHEN reviews.overall  = 2 THEN 1
</span><span class=s>              WHEN reviews.overall  = 3 THEN 2
</span><span class=s>              WHEN reviews.overall  = 4 THEN 3
</span><span class=s>              WHEN reviews.overall  = 5 THEN 3
</span><span class=s>       END AS label
</span><span class=s>      ,reviews.reviewText AS text
</span><span class=s>      ,row_number() OVER (PARTITION BY overall ORDER BY rand()) AS rowNumber
</span><span class=s>    FROM reviews
</span><span class=s>  ) reviews
</span><span class=s>  WHERE rowNumber &lt;= </span><span class=si>$datasize</span><span class=s>
</span><span class=s>  &#34;&#34;&#34;</span>
<span class=o>)</span>

<span class=o>+-----+-----+</span>
<span class=o>|</span><span class=n>label</span><span class=o>|</span><span class=n>count</span><span class=o>|</span>
<span class=o>+-----+-----+</span>
<span class=o>|</span>    <span class=mi>1</span><span class=o>|</span><span class=mi>40000</span><span class=o>|</span>
<span class=o>|</span>    <span class=mi>2</span><span class=o>|</span><span class=mi>20000</span><span class=o>|</span>
<span class=o>|</span>    <span class=mi>3</span><span class=o>|</span><span class=mi>40000</span><span class=o>|</span>
<span class=o>+-----+-----+</span></code></pre></div><p>Based on just the two label option <code>1</code> and <code>5</code> with the initial 20000/label dataset we get these results which are much better (87.52%) and with only 40000 reviews so we should still be able to get better results by increasing the <code>training</code> set size (where I managed to reach around 90.5%). This is expected we are taking the extreme reviews where we should see the largest discrepancies between reviews (it is unlikely that someone will use words like <code>excellent</code> or <code>incredible</code> in a review they give a score of <code>1</code> to).</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Precision</span><span class=k>:</span> <span class=kt>Double</span> <span class=o>=</span> <span class=mf>0.8752</span>
<span class=nc>Test</span> <span class=nc>Error</span> <span class=k>=</span> <span class=mf>0.12480000000000002</span>
<span class=nc>Duration</span><span class=k>:</span> <span class=err>1155037</span> <span class=kt>ms</span> <span class=o>(</span><span class=kt>~</span><span class=err>19</span> <span class=kt>minutes</span><span class=o>)</span></code></pre></div><p>Based on just the three label &lsquo;flattened&rsquo; option with <code>1</code> <code>2</code> <code>3</code> with the initial 20000/label dataset we get these results which are also much better (65.88%) than the 44.10% baseline. I have tested larger datasets and manage to get &gt; 70% accuracy with this method with larger data sets. In this approach we are normalising the <code>good</code>, <code>bad</code> or <code>indifferent</code> reviews where someone giving a <code>4</code> might use very similar language as a review with a score of <code>5</code>.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Precision</span><span class=k>:</span> <span class=kt>Double</span> <span class=o>=</span> <span class=mf>0.65888</span>
<span class=nc>Test</span> <span class=nc>Error</span> <span class=k>=</span> <span class=mf>0.34112</span>
<span class=nc>Duration</span><span class=k>:</span> <span class=err>1897177</span> <span class=kt>ms</span> <span class=o>(</span><span class=kt>~</span><span class=err>31</span> <span class=kt>minutes</span><span class=o>)</span></code></pre></div><p>At this point it makes sense to think about the use case of this data and what accuracy needs to be reached. One obvious approach would be to run as much of the 22 million reviews through the model and test as many parameter sets as possible (a brute force approach), perform additional feature extraction or switch to an alternative model to Naive Bayes.</p><h5 id=5-3-what-about-overfitting>5.3 What about overfitting?</h5><p>The image above also shows a red line which gets better as the model gets more complex then actually performs worse as the model complexity reaches a certain level. This is an outcome of overfitting or building a model which fits the training data very well but doesn&rsquo;t perform against actual test data.</p><p>You can imagine a very simple example of overfitting as:
- we need a model which predicts gender from height of person
- we accidentally set our training data to a set which only contains females
- we think we have an amazingly accurate model as it works 95% of the time on the training set
- the model actually performs very poorly when we expand the data to include both genders</p><p>Spark ML overcomes a lot of this problem by providing the <code>ParamGridBuilder()</code> and the <code>CrossValidator()</code> that we defined earlier:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>paramGrid</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>ParamGridBuilder</span><span class=o>()</span>
  <span class=o>.</span><span class=n>addGrid</span><span class=o>(</span><span class=n>removerHashingTF</span><span class=o>.</span><span class=n>numFeatures</span><span class=o>,</span> <span class=nc>Array</span><span class=o>(</span><span class=mi>1000</span><span class=o>,</span><span class=mi>10000</span><span class=o>))</span>
  <span class=o>.</span><span class=n>addGrid</span><span class=o>(</span><span class=n>ngram2HashingTF</span><span class=o>.</span><span class=n>numFeatures</span><span class=o>,</span> <span class=nc>Array</span><span class=o>(</span><span class=mi>1000</span><span class=o>,</span><span class=mi>10000</span><span class=o>))</span>
  <span class=o>.</span><span class=n>addGrid</span><span class=o>(</span><span class=n>ngram3HashingTF</span><span class=o>.</span><span class=n>numFeatures</span><span class=o>,</span> <span class=nc>Array</span><span class=o>(</span><span class=mi>1000</span><span class=o>,</span><span class=mi>10000</span><span class=o>))</span>
  <span class=o>.</span><span class=n>build</span><span class=o>()</span>
<span class=o>}</span></code></pre></div><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>cv</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>CrossValidator</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setEstimator</span><span class=o>(</span><span class=n>pipeline</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setEvaluator</span><span class=o>(</span><span class=k>new</span> <span class=nc>MulticlassClassificationEvaluator</span><span class=o>().</span><span class=n>setLabelCol</span><span class=o>(</span><span class=s>&#34;indexedLabel&#34;</span><span class=o>).</span><span class=n>setPredictionCol</span><span class=o>(</span><span class=s>&#34;prediction&#34;</span><span class=o>).</span><span class=n>setMetricName</span><span class=o>(</span><span class=s>&#34;precision&#34;</span><span class=o>))</span>
  <span class=o>.</span><span class=n>setEstimatorParamMaps</span><span class=o>(</span><span class=n>paramGrid</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setNumFolds</span><span class=o>(</span><span class=mi>2</span><span class=o>)</span> <span class=c1>// Use 3+ in practice
</span><span class=c1></span><span class=o>}</span></code></pre></div><h6 id=5-3-1-paramgridbuider>5.3.1 ParamGridBuider()</h6><p>Based on how the <code>HashingTF()</code> transformer works we would expect that if we add more features we will get closer to a 1:1 word:hash ratio and surely having more features will help right?</p><p>Actually, no. Beyond a certain point adding features actually makes the model perform worse against the training data. My best performing basic model actually finds that <code>1000</code> 3-gram features actually outperforms <code>10000</code> 3-gram features due to overfitting.</p><p>How do I know?</p><p>We interrogate the <code>CrossValidator().bestModel</code> which is actually a <code>PipelineModel</code>. The only annoying thing is that we are finding the correct stage by using its index (<code>stages(5)</code>, etc.).</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>bestPipelineModel</span> <span class=k>=</span> <span class=n>cvModel</span><span class=o>.</span><span class=n>bestModel</span><span class=o>.</span><span class=n>asInstanceOf</span><span class=o>[</span><span class=kt>PipelineModel</span><span class=o>]</span>
<span class=k>val</span> <span class=n>stages</span> <span class=k>=</span> <span class=n>bestPipelineModel</span><span class=o>.</span><span class=n>stages</span>
<span class=k>val</span> <span class=n>removerHashingTFStage</span> <span class=k>=</span> <span class=n>stages</span><span class=o>(</span><span class=mi>5</span><span class=o>).</span><span class=n>asInstanceOf</span><span class=o>[</span><span class=kt>HashingTF</span><span class=o>]</span>
<span class=k>val</span> <span class=n>ngram2HashingTFStage</span> <span class=k>=</span> <span class=n>stages</span><span class=o>(</span><span class=mi>6</span><span class=o>).</span><span class=n>asInstanceOf</span><span class=o>[</span><span class=kt>HashingTF</span><span class=o>]</span>
<span class=k>val</span> <span class=n>ngram3HashingTFStage</span> <span class=k>=</span> <span class=n>stages</span><span class=o>(</span><span class=mi>7</span><span class=o>).</span><span class=n>asInstanceOf</span><span class=o>[</span><span class=kt>HashingTF</span><span class=o>]</span>

<span class=n>println</span><span class=o>(</span><span class=s>&#34;Best Model Parameters:&#34;</span><span class=o>)</span>
<span class=n>println</span><span class=o>(</span><span class=s>&#34;removerHashingTF numFeatures = &#34;</span> <span class=o>+</span> <span class=n>removerHashingTFStage</span><span class=o>.</span><span class=n>getNumFeatures</span><span class=o>)</span>
<span class=n>println</span><span class=o>(</span><span class=s>&#34;ngram2HashingTFStage numFeatures = &#34;</span> <span class=o>+</span> <span class=n>ngram2HashingTFStage</span><span class=o>.</span><span class=n>getNumFeatures</span><span class=o>)</span>
<span class=n>println</span><span class=o>(</span><span class=s>&#34;ngram3HashingTFStage numFeatures = &#34;</span> <span class=o>+</span> <span class=n>ngram3HashingTFStage</span><span class=o>.</span><span class=n>getNumFeatures</span><span class=o>)</span></code></pre></div><p>Which produces:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Best</span> <span class=nc>Model</span> <span class=nc>Parameters</span><span class=k>:</span>
<span class=kt>removerHashingTF</span> <span class=kt>numFeatures</span> <span class=o>=</span> <span class=mi>10000</span>
<span class=n>ngram2HashingTFStage</span> <span class=n>numFeatures</span> <span class=k>=</span> <span class=mi>10000</span>
<span class=n>ngram3HashingTFStage</span> <span class=n>numFeatures</span> <span class=k>=</span> <span class=mi>1000</span></code></pre></div><h6 id=5-3-2-crossvalidator>5.3.2 CrossValidator</h6><p>The other useful feature to help prevent overfitting is <code>CrossValidator().setNumFolds(n)</code>. Once again WikiPedia comes through with a <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>good explaination</a> but basically the number of folds is are the number of subsets the validator will create in your training data to limit overfitting.</p><p>So if we have <code>.setNumFolds(2)</code> the validator will perform these steps:</p><ul><li>randomly split the model into two equal subsets</li><li>build all the models based on the <code>ParamGridBuilder()</code> array on the first of two subsets of the training data and will test against the second subset</li><li>build all the models based on the <code>ParamGridBuilder()</code> array on the second of the two subsets and test against the first subset</li><li>pick the model with the best fit across all of these runs and subsets</li></ul><p>So you can see that if we set:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>paramGrid</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>ParamGridBuilder</span><span class=o>()</span>
  <span class=o>.</span><span class=n>addGrid</span><span class=o>(</span><span class=n>removerHashingTF</span><span class=o>.</span><span class=n>numFeatures</span><span class=o>,</span> <span class=nc>Array</span><span class=o>(</span><span class=mi>1000</span><span class=o>,</span><span class=mi>10000</span><span class=o>))</span>
  <span class=o>.</span><span class=n>addGrid</span><span class=o>(</span><span class=n>ngram2HashingTF</span><span class=o>.</span><span class=n>numFeatures</span><span class=o>,</span> <span class=nc>Array</span><span class=o>(</span><span class=mi>1000</span><span class=o>,</span><span class=mi>10000</span><span class=o>))</span>
  <span class=o>.</span><span class=n>addGrid</span><span class=o>(</span><span class=n>ngram3HashingTF</span><span class=o>.</span><span class=n>numFeatures</span><span class=o>,</span> <span class=nc>Array</span><span class=o>(</span><span class=mi>1000</span><span class=o>,</span><span class=mi>10000</span><span class=o>))</span>
  <span class=o>.</span><span class=n>build</span><span class=o>()</span>
<span class=o>}</span></code></pre></div><p>which results in 3x2 or 6 combinations and set the number of folds to the recommended minimum of 3:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=o>.</span><span class=n>setNumFolds</span><span class=o>(</span><span class=mi>3</span><span class=o>)</span></code></pre></div><p>Find ourselves running the model trainer 18 times&hellip; which is the cost of limiting overfitting.</p><h4 id=6-next-steps>6 Next Steps</h4><p>So we have built an end-to-end classifier based on Amazon book reviews and reached at best 90.5% accuracy and as low as 46%. The next best step would be to try a brute force attempt with a much larger training set and a very large list of parameters for each transformer.</p><p>I think I will try alternative classification algorithms (not NaiveBayes) but that is a topic for another post.</p></div><div id=source>If you find an error please raise a <a class="basic-alignment left" href=https://github.com/seddonm1/seddonm1.github.io/blob/main/content/posts/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-2.md>pull request</a>.</div><div id=links><a class="basic-alignment left" href=https://reorchestrate.com/posts/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/>&laquo; Natural Language Processing with Apache Spark ML and Amazon Reviews (Part 1)</a>
<a class="basic-alignment left" href=https://reorchestrate.com/posts/porter-stemming-in-apache-spark-ml/>Porter Stemming in Apache Spark ML &raquo;</a></div></section></body></html>