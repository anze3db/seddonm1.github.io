<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content=True><meta name=MobileOptimized content=320><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content=no-referrer><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400" rel=stylesheet type=text/css><link rel=icon type=image/png href=https://reorchestrate.com/favicon_16x16.png sizes=16x16><link rel=icon type=image/png href=https://reorchestrate.com/favicon_32x32.png sizes=32x32><link rel=icon type=image/png href=https://reorchestrate.com/favicon_128x128.png sizes=128x128><title>Performance Tuning Spark WikiPedia PageRank</title><link rel=canonical href=https://reorchestrate.com/posts/performance-tuning-spark-pagerank/><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;-webkit-font-smoothing:antialiased;line-height:160%;color:#1d1313;max-width:950px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:#0000;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3,h4,h5{font-family:old standard tt,serif}strong{font-weight:600}table{width:100%}thead{font-weight:800;background:#dadada}tbody tr:nth-child(even){background:#f7f7f7}table td,th{padding:2px}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-weight:700;font-size:35px}h2{font-weight:700;font-size:28px}h3{font-weight:700;font-size:22px;margin-top:28px}h4{font-weight:700;font-size:20px;margin-top:20px}h5{font-size:18px;margin-top:18px}h1 a,h2 a,h3 a,h4 a,h5 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,time{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content time{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#source{margin:50px 0 0}#links{margin:50px 0 0}#links :nth-child(2){float:right}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}#nav .sub{font-size:80%;display:inline-block;vertical-align:middle}#nav .sub p{color:#403c3b;margin:0}#nav .sub a{text-decoration:none}#nav .sub svg{fill:#403c3b;display:inline-block;vertical-align:middle;padding-bottom:3px;padding-right:5px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}.chroma{background-color:#f0f0f0}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em}.chroma .ln{margin-right:.4em;padding:0 .4em}.chroma .k{color:#007020;font-weight:700}.chroma .kc{color:#007020;font-weight:700}.chroma .kd{color:#007020;font-weight:700}.chroma .kn{color:#007020;font-weight:700}.chroma .kp{color:#007020}.chroma .kr{color:#007020;font-weight:700}.chroma .kt{color:#902000}.chroma .na{color:#4070a0}.chroma .nb{color:#007020}.chroma .nc{color:#0e84b5;font-weight:700}.chroma .no{color:#60add5}.chroma .nd{color:#555;font-weight:700}.chroma .ni{color:#d55537;font-weight:700}.chroma .ne{color:#007020}.chroma .nf{color:#06287e}.chroma .nl{color:#002070;font-weight:700}.chroma .nn{color:#0e84b5;font-weight:700}.chroma .nt{color:#062873;font-weight:700}.chroma .nv{color:#bb60d5}.chroma .s{color:#4070a0}.chroma .sa{color:#4070a0}.chroma .sb{color:#4070a0}.chroma .sc{color:#4070a0}.chroma .dl{color:#4070a0}.chroma .sd{color:#4070a0;font-style:italic}.chroma .s2{color:#4070a0}.chroma .se{color:#4070a0;font-weight:700}.chroma .sh{color:#4070a0}.chroma .si{color:#70a0d0;font-style:italic}.chroma .sx{color:#c65d09}.chroma .sr{color:#235388}.chroma .s1{color:#4070a0}.chroma .ss{color:#517918}.chroma .m{color:#40a070}.chroma .mb{color:#40a070}.chroma .mf{color:#40a070}.chroma .mh{color:#40a070}.chroma .mi{color:#40a070}.chroma .il{color:#40a070}.chroma .mo{color:#40a070}.chroma .o{color:#666}.chroma .ow{color:#007020;font-weight:700}.chroma .c{color:#60a0b0;font-style:italic}.chroma .ch{color:#60a0b0;font-style:italic}.chroma .cm{color:#60a0b0;font-style:italic}.chroma .c1{color:#60a0b0;font-style:italic}.chroma .cs{color:#60a0b0;background-color:#fff0f0}.chroma .cp{color:#007020}.chroma .cpf{color:#007020}.chroma .gd{color:#a00000}.chroma .ge{font-style:italic}.chroma .gr{color:red}.chroma .gh{color:navy;font-weight:700}.chroma .gi{color:#00a000}.chroma .go{color:#888}.chroma .gp{color:#c65d09;font-weight:700}.chroma .gs{font-weight:700}.chroma .gu{color:purple;font-weight:700}.chroma .gt{color:#04d}.chroma .w{color:#bbb}</style></head><body><section id=nav><h1><a href=https://reorchestrate.com/>reorchestrate</a></h1><section class=section><div class=sub><p>&copy; Mike Seddon 2023 |
<a href=https://www.github.com/seddonm1 target=_blank><svg width="22" height="22" viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M896 128q209 0 385.5 103T1561 510.5 1664 896q0 251-146.5 451.5T1139 1625q-27 5-40-7t-13-30q0-3 .5-76.5t.5-134.5q0-97-52-142 57-6 102.5-18t94-39 81-66.5 53-105T1386 856q0-119-79-206 37-91-8-204-28-9-81 11t-92 44l-38 24q-93-26-192-26t-192 26q-16-11-42.5-27T578 459.5 493 446q-45 113-8 204-79 87-79 206 0 85 20.5 150t52.5 105 80.5 67 94 39 102.5 18q-39 36-49 103-21 10-45 15t-57 5-65.5-21.5T484 1274q-19-32-48.5-52t-49.5-24l-20-3q-21 0-29 4.5t-5 11.5 9 14 13 12l7 5q22 10 43.5 38t31.5 51l10 23q13 38 44 61.5t67 30 69.5 7 55.5-3.5l23-4q0 38 .5 88.5t.5 54.5q0 18-13 30t-40 7q-232-77-378.5-277.5T128 896q0-209 103-385.5T510.5 231 896 128zM419 1231q3-7-7-12-10-3-13 2-3 7 7 12 9 6 13-2zm31 34q7-5-2-16-10-9-16-3-7 5 2 16 10 10 16 3zm30 45q9-7 0-19-8-13-17-6-9 5 0 18t17 7zm42 42q8-8-4-19-12-12-20-3-9 8 4 19 12 12 20 3zm57 25q3-11-13-16-15-4-19 7t13 15q15 6 19-6zm63 5q0-13-17-11-16 0-16 11 0 13 17 11 16 0 16-11zm58-10q-2-11-18-9-16 3-14 15t18 8 14-14z"/></svg></a><a href=https://www.linkedin.com/in/mikeseddonau/ target=_blank><svg width="22" height="22" viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M365 1414h231V720H365v694zm246-908q-1-52-36-86t-93-34-94.5 34-36.5 86q0 51 35.5 85.5T479 626h1q59 0 95-34.5t36-85.5zm585 908h231v-398q0-154-73-233t-193-79q-136 0-209 117h2V720H723q3 66 0 694h231v-388q0-38 7-56 15-35 45-59.5t74-24.5q116 0 116 157v371zm468-998v960q0 119-84.5 203.5T1376 1664H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z"/></svg></a></p></div></section><ul></ul></section><section id=content><h1>Performance Tuning Spark WikiPedia PageRank</h1><div id=sub-header>21 November 2015</div><div class=entry-content><p>In my previous post I wrote some code to demonstrate how to go from the raw database extracts provided monthly by WikiPedia through to loading into Apache Spark GraphX and running PageRank.</p><p>In this post I will discuss my efforts to make that process more efficient which may be relevant to some of you trying to do proof-of-concept activities on less than ideal hardware. My test box (a standalone Intel Core i3-3217U (17W) with 16GB RAM and 60GB SSD storage) cannot complete the full graph build due to insufficient resources. At most I can process around 10-20% of the dataset before hitting these resource constraints.</p><p>To allow the process to complete, I have generated a &lsquo;random&rsquo; subgraph of 2682475 vertices and 22751309 edges by altering the regex to extract just <code>page_id</code> and <code>pl_from</code> starting with 2:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>pagePattern</span> <span class=k>=</span> <span class=s>&#34;\\(([2]\\d*),0,&#39;([^,]*?)&#39;,&#39;.*?\\)+[,|;]&#34;</span><span class=o>.</span><span class=n>r</span>
<span class=k>val</span> <span class=n>pagelinksPattern</span> <span class=k>=</span> <span class=s>&#34;\\(([2]\\d*),0,&#39;([^,]*?)&#39;,0\\)+[,|;]&#34;</span><span class=o>.</span><span class=n>r</span></code></pre></div><p>On the <code>pagelinks</code> side we will extract far more records than we initially need (77730564) because we do not validate whether the target appears in the <code>pageRDD</code> however these will be removed by the <code>join()</code> operation.</p><p>Running this with my previous code yields a quite consistent runtime using GZIP. This will be our benchmark for comparison.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Duration</span><span class=k>:</span> <span class=err>2132462</span> <span class=kt>ms</span> <span class=o>(</span><span class=kt>~</span><span class=err>35</span> <span class=kt>minutes</span><span class=o>)</span>
<span class=kt>Duration:</span> <span class=err>2110173</span> <span class=kt>ms</span> <span class=o>(</span><span class=kt>~</span><span class=err>35</span> <span class=kt>minutes</span><span class=o>)</span></code></pre></div><h3 id=1-data-compression>1. Data Compression</h3><p>I wanted to include some analysis on data compression as the results were surprising to me.</p><p>WikiPedia provides the extracts as GZIP files which can be read by the <code>SparkContext.textFile()</code> method out of the box. This is very useful as the <code>pagelinks</code> data set is 4.7GB GZIP compressed and over 35GB uncompressed. I do not have the luxury of that much disk space and even if I did I don&rsquo;t really want to store all the raw data as it forces increased disk IO.</p><table><tr><th>Algorithm</th><th>Splittable</th><th>Compressed Size</th><th>Job Duration</th></tr><tr><td>GZIP</td><td>No</td><td>5.78GB</td><td>2132462 ms (~35 minutes)</td></tr><tr><td>BZIP2</td><td>Yes</td><td>4.78GB</td><td>2926875 ms (~48 minutes)</td></tr><tr><td>Snappy</td><td>Yes*</td><td>9.40GB</td><td>1561278 ms (~26 minutes)</td></tr><tr><td>LZ4</td><td>Yes</td><td>9.19GB</td><td>DNF</td></tr></table><h4 id=1-1-bzip2>1.1. BZIP2</h4><p>I have my Spark instance to utilise all 4 threads provided by my CPU (2 executors with 2 threads each). When running GZIP due to it&rsquo;s non-splittable nature I can only see the job being executed as one task on one executor. When running the BZIP2 file which is splittable I can see four tasks being executed in parallel. I assumed that because I was allowing multi-threading the BZIP2 extract would be much faster but based on the result above it seems that the CPU overhead of the BZIP2 compression algorithm is so great that even multithreading cannot overcome it on my hardware.</p><h4 id=1-2-snappy>1.2. Snappy</h4><p>I also tested Snappy compression which you can see performed the best of the three but came with the painful process of building native libraries for <a href=https://google.github.io/snappy/>snappy</a>, <a href=https://github.com/electrum/hadoop-snappy>hadoop-snappy</a> and <a href=https://github.com/liancheng/snappy-utils>snappy-utils</a> so that I could compress in the &lsquo;correct&rsquo; <code>hadoop-snappy</code> format.</p><h4 id=1-3-lz4>1.3. LZ4</h4><p>I could not get LZ4 compression to work but I think it will be easier in later Spark releases. I think this could be a nice solution as it has comparable performance to Snappy but doesn&rsquo;t have the confusion about snappy vs. hadoop-snappy and is already easily installed on most platforms by just a simple <code>apt-get</code>.</p><h4 id=1-4-having-to-recompress-probably-takes-away-these-benefits>1.4. Having to recompress probably takes away these benefits</h4><p>The other issue is that to do this testing I had to decompress the GZIP files and recompress as BZIP2 and Snappy. Both these processes take considerable time which is not included in my Job Duration above. By providing the files as GZIP WikiPedia has already contributed substantial CPU time which is not included in any of these job times.</p><h3 id=2-caching-and-serialisation>2. Caching and Serialisation</h3><p>One of the beautiful things about Spark is the ability to cache datasets in memory. If you think about the process described to calculate PageRank there are some very CPU intensive processes being carried out that we really don&rsquo;t want to have to perform more than once:</p><ul><li>Data decompression</li><li>Extracting data from each line by regex pattern matching</li><li>Joining data</li><li>Building the graph</li></ul><p>Spark, by default, will cache objects in <code>MEMORY_ONLY</code> as deserialised Java objects so that these two commands do the same thing:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>pageRDD</span><span class=o>.</span><span class=n>cache</span><span class=o>()</span> 
<span class=n>pageRDD</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY</span><span class=o>)</span></code></pre></div><p>Spark automatically monitors cache usage on each node and drops out old data partitions in a least-recently-used (LRU) fashion. This means that if we tell Spark to <code>cache()</code> our initial <code>pageRDD</code> dataset after it has been decompressed and done the regex pattern matching it will by default store it in memory as <code>MEMORY_ONLY</code>. This is great if we have enough RAM to store all the data we want to <code>cache()</code> but if we run big jobs we can quickly run out and the node will drop the &lsquo;old&rsquo; (but possibly still relevant) data.</p><p>So what happens when our old dataset gets removed from RAM and we need it downstream? <strong>Spark recalculates it</strong>. This means that when I call the graph constructor and it generates a <em>lot</em> of additional cached RDDs it may flush out my original <code>pageRDD</code> and <code>pagelinksRDD</code>. In worst case Spark may have to go back and re-extract the data. As you can imagine this is a performance destroyer.</p><p>To avoid heavy recalculation an option is available to store datasets either spill to disk or use <code>serialised</code> datasets which take up less space in memory (which might allow you to store all the data in memory thus avoiding this problem) at the expense of increased CPU time or to allow the memory to spill to disk at the expense of disk IO and obviously lower access speed than RAM. The question is whether the cost of disk IO is less than the CPU cost of recalculation.</p><h4 id=2-1-memory-only-ser>2.1. <code>MEMORY_ONLY_SER</code></h4><p>Spark suggests the first approach should be to try <code>StorageLevel.MEMORY_ONLY_SER</code> which will make the datasets much smaller and hopefully allow them to be kept in memory. The trade-off is that CPU time is spent serialising and deserialising the datasets.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>pageRDD</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY_SER</span><span class=o>)</span>
<span class=n>pageRDD</span><span class=o>.</span><span class=n>count</span></code></pre></div><p>With this code applied to the <code>pageRdd</code>, <code>pagelinksRDD</code>, <code>edges</code> and <code>vertices</code> RDDs I actually had <em>worse</em> performance by 8.2% which must be due to the overhead of serialising and deserialising the datasets.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>pageRDD</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY_SER</span><span class=o>)</span>
<span class=n>pagelinksRDD</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY_SER</span><span class=o>)</span>
<span class=n>edges</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY_SER</span><span class=o>)</span>
<span class=n>vertices</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY_SER</span><span class=o>)</span>

<span class=nc>Duration</span><span class=k>:</span> <span class=err>2307983</span> <span class=kt>ms</span> <span class=o>(</span><span class=kt>~</span><span class=err>38</span> <span class=kt>minutes</span><span class=o>)</span> <span class=o>(</span><span class=err>108</span><span class=kt>.</span><span class=err>2</span><span class=kt>%</span><span class=o>)</span></code></pre></div><h4 id=2-2-memory-and-disk>2.2. <code>MEMORY_AND_DISK</code></h4><p>By watching the Storage tab in the application UI whilst the job is running I determined that the <code>pagelinksRDD</code> was not being stored in memory which meant heavy CPU time doing recalculation. My solution was to work out how I could cache as much as possible in RAM and spill over any additional data to disk which the <code>MEMORY_AND_DISK</code> option provides.</p><p>What is very nice about <code>MEMORY_AND_DISK</code> is that when the cache is full and more data is requested to be cached it actually spills those existing cached partitions to the disk as serialised objects - that means worst case upstream recalculations are not required. The downside of this is that there is CPU time spent serialising the objects being spilled to disk and also disk write time and storage requirements.</p><p>Here is a screenshot showing my datasets sitting partially in RAM and partially on disk:
<img src=https://reorchestrate.com/img/2015/11/SparkMemory.PNG alt="Data cached to memory and disk"></p><p>The best approach that I found was to determine from the program flow at which point those datasets would not be needed and manually <code>unpersist()</code> them:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>pageRDD</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_AND_DISK</span><span class=o>)</span>
<span class=n>pagelinksRDD</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_AND_DISK</span><span class=o>)</span>
<span class=n>edges</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_AND_DISK</span><span class=o>)</span>

<span class=c1>// after the pageRDD to pagelinksRDD join is done we don&#39;t need pagelinksRDD
</span><span class=c1></span><span class=n>pagelinksRDD</span><span class=o>.</span><span class=n>unpersist</span><span class=o>()</span>

<span class=n>vertices</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_AND_DISK</span><span class=o>)</span>

<span class=c1>// after the pageRDD to vertices mapping is done we don&#39;t need pageRDD
</span><span class=c1></span><span class=n>pageRDD</span><span class=o>.</span><span class=n>unpersist</span><span class=o>()</span>

<span class=nc>Duration</span><span class=k>:</span> <span class=err>2155781</span> <span class=kt>ms</span> <span class=o>(</span><span class=kt>~</span><span class=err>35</span> <span class=kt>minutes</span><span class=o>)</span> <span class=o>(</span><span class=err>100</span><span class=kt>%</span><span class=o>)</span></code></pre></div><p>This approach is more scalable as it will utilise maximum available RAM and prevent recalculation assuming you have fast storage attached to the executors which is faster than recalculating the datasets.</p><h4 id=2-3-partitionby>2.3. <code>partitionBy()</code></h4><p>The final interesting thing I worked out was that due to the fact that Spark actually caches partitions not RDDs it may be beneficial to <code>partitionBy()</code> the datasets to cache as much as possible into RAM and spill any remaining data to disk. Because we are loading the GZIP files by default the <code>pageRDD</code> and <code>pagelinksRDD</code> will be single partitioned as Spark cannot store half a partition in RAM and half on disk.</p><p>To increase the number of partitions:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>import</span> <span class=nn>org.apache.spark.HashPartitioner</span>

<span class=k>val</span> <span class=n>pageRDD</span><span class=k>:</span> <span class=kt>RDD</span><span class=o>[(</span><span class=kt>String</span>, <span class=kt>Long</span><span class=o>)]</span> <span class=k>=</span> <span class=n>page</span><span class=o>.</span><span class=n>flatMap</span> <span class=o>{</span> <span class=n>line</span> <span class=k>=&gt;</span>
  <span class=k>val</span> <span class=n>matches</span> <span class=k>=</span> <span class=n>pagePattern</span><span class=o>.</span><span class=n>findAllMatchIn</span><span class=o>(</span><span class=n>line</span><span class=o>).</span><span class=n>toArray</span>
  <span class=n>matches</span><span class=o>.</span><span class=n>map</span> <span class=o>{</span> <span class=n>d</span> <span class=k>=&gt;</span>
    <span class=c1>// &#34;page_title&#34;,&#34;page_id&#34;
</span><span class=c1></span>    <span class=o>(</span><span class=n>d</span><span class=o>.</span><span class=n>group</span><span class=o>(</span><span class=mi>2</span><span class=o>),</span> <span class=n>d</span><span class=o>.</span><span class=n>group</span><span class=o>(</span><span class=mi>1</span><span class=o>).</span><span class=n>toLong</span><span class=o>)</span>
  <span class=o>}</span>
<span class=o>}.</span><span class=n>partitionBy</span><span class=o>(</span><span class=k>new</span> <span class=nc>HashPartitioner</span><span class=o>(</span><span class=mi>10</span><span class=o>)).</span><span class=n>setName</span><span class=o>(</span><span class=s>&#34;pageRDD&#34;</span><span class=o>).</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_AND_DISK</span><span class=o>)</span>
<span class=n>pageRDD</span><span class=o>.</span><span class=n>count</span></code></pre></div><p>This means there will be 10 partitions in the <code>pageRDD</code> dataset which are derived by calculating a hash of the first field (<code>page_title</code> in this case). My understanding is that because we call <code>HashPartitioner</code> on both the <code>pageRDD</code> and <code>pagelinksRDD</code> that they will be partitioned by the same keys which should result in less shuffle operations for the <code>join()</code> stage resulting in improved performance - particularly when the larger datasets are considered. The partitioner does come with a downside of having to calculate the hash (again CPU bound) for each record which has resulted in decreased performance on my platform.</p><h4 id=2-4-best-performer-memory-only-and-memory-and-disk>2.4. Best Performer: <code>MEMORY_ONLY</code> and <code>MEMORY_AND_DISK</code></h4><p>For me the best performance for my WikiPedia PageRank sub-graph was to do a hybrid of these approaches.</p><p>Ultimately my code behaved best on my subset of the WikiPedia graph without <code>paritionBy()</code> without allowing spill to disk on my two key datasets <code>edges</code> and <code>vertices</code> (and as the will be heavily used to create the <code>graph</code> they will not be evicted from memory).</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>pageRDD</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY</span><span class=o>)</span>
<span class=n>pagelinksRDD</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_AND_DISK</span><span class=o>)</span>
<span class=n>edges</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY</span><span class=o>)</span>
<span class=n>pagelinksRDD</span><span class=o>.</span><span class=n>unpersist</span><span class=o>()</span>
<span class=n>vertices</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY</span><span class=o>)</span>
<span class=n>pageRDD</span><span class=o>.</span><span class=n>unpersist</span><span class=o>()</span>

<span class=nc>Duration</span><span class=k>:</span> <span class=err>1662171</span> <span class=kt>ms</span> <span class=o>(</span><span class=kt>~</span><span class=err>27</span> <span class=kt>minutes</span><span class=o>)</span> <span class=o>(</span><span class=err>77</span><span class=kt>.</span><span class=err>9</span><span class=kt>%</span><span class=o>)</span></code></pre></div><h3 id=3-summary>3. Summary</h3><p>Hopefully if you have read this far this guide highlights one thing to you: <strong>there is no best way of optimising this for all architectures</strong> and you need to test these yourself in your environment with your data/code.</p></div><div id=source>If you find an error please raise a <a class="basic-alignment left" href=https://github.com/seddonm1/seddonm1.github.io/blob/main/content/posts/performance-tuning-spark-pagerank.md>pull request</a>.</div><div id=links><a class="basic-alignment left" href=https://reorchestrate.com/posts/computing-wikipedias-internal-pagerank-with-spark/>&laquo; Computing WikiPedia&#39;s internal PageRank with Apache Spark</a>
<a class="basic-alignment left" href=https://reorchestrate.com/posts/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/>Natural Language Processing with Apache Spark ML and Amazon Reviews (Part 1) &raquo;</a></div></section></body></html>