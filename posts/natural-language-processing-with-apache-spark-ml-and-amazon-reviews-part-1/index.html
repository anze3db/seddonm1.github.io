<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content=True><meta name=MobileOptimized content=320><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content=no-referrer><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400" rel=stylesheet type=text/css><link rel=icon type=image/png href=https://reorchestrate.com/favicon_16x16.png sizes=16x16><link rel=icon type=image/png href=https://reorchestrate.com/favicon_32x32.png sizes=32x32><link rel=icon type=image/png href=https://reorchestrate.com/favicon_128x128.png sizes=128x128><title>Natural Language Processing with Apache Spark ML and Amazon Reviews (Part 1)</title><link rel=canonical href=https://reorchestrate.com/posts/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;-webkit-font-smoothing:antialiased;line-height:160%;color:#1d1313;max-width:950px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:#0000;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3,h4,h5{font-family:old standard tt,serif}strong{font-weight:600}table{width:100%}thead{font-weight:800;background:#dadada}tbody tr:nth-child(even){background:#f7f7f7}table td,th{padding:2px}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-weight:700;font-size:35px}h2{font-weight:700;font-size:28px}h3{font-weight:700;font-size:22px;margin-top:28px}h4{font-weight:700;font-size:20px;margin-top:20px}h5{font-size:18px;margin-top:18px}h1 a,h2 a,h3 a,h4 a,h5 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,time{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content time{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#source{margin:50px 0 0}#links{margin:50px 0 0}#links :nth-child(2){float:right}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}#nav .sub{font-size:80%;display:inline-block;vertical-align:middle}#nav .sub p{color:#403c3b;margin:0}#nav .sub a{text-decoration:none}#nav .sub svg{fill:#403c3b;display:inline-block;vertical-align:middle;padding-bottom:3px;padding-right:5px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}.chroma{background-color:#f0f0f0}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em}.chroma .ln{margin-right:.4em;padding:0 .4em}.chroma .k{color:#007020;font-weight:700}.chroma .kc{color:#007020;font-weight:700}.chroma .kd{color:#007020;font-weight:700}.chroma .kn{color:#007020;font-weight:700}.chroma .kp{color:#007020}.chroma .kr{color:#007020;font-weight:700}.chroma .kt{color:#902000}.chroma .na{color:#4070a0}.chroma .nb{color:#007020}.chroma .nc{color:#0e84b5;font-weight:700}.chroma .no{color:#60add5}.chroma .nd{color:#555;font-weight:700}.chroma .ni{color:#d55537;font-weight:700}.chroma .ne{color:#007020}.chroma .nf{color:#06287e}.chroma .nl{color:#002070;font-weight:700}.chroma .nn{color:#0e84b5;font-weight:700}.chroma .nt{color:#062873;font-weight:700}.chroma .nv{color:#bb60d5}.chroma .s{color:#4070a0}.chroma .sa{color:#4070a0}.chroma .sb{color:#4070a0}.chroma .sc{color:#4070a0}.chroma .dl{color:#4070a0}.chroma .sd{color:#4070a0;font-style:italic}.chroma .s2{color:#4070a0}.chroma .se{color:#4070a0;font-weight:700}.chroma .sh{color:#4070a0}.chroma .si{color:#70a0d0;font-style:italic}.chroma .sx{color:#c65d09}.chroma .sr{color:#235388}.chroma .s1{color:#4070a0}.chroma .ss{color:#517918}.chroma .m{color:#40a070}.chroma .mb{color:#40a070}.chroma .mf{color:#40a070}.chroma .mh{color:#40a070}.chroma .mi{color:#40a070}.chroma .il{color:#40a070}.chroma .mo{color:#40a070}.chroma .o{color:#666}.chroma .ow{color:#007020;font-weight:700}.chroma .c{color:#60a0b0;font-style:italic}.chroma .ch{color:#60a0b0;font-style:italic}.chroma .cm{color:#60a0b0;font-style:italic}.chroma .c1{color:#60a0b0;font-style:italic}.chroma .cs{color:#60a0b0;background-color:#fff0f0}.chroma .cp{color:#007020}.chroma .cpf{color:#007020}.chroma .gd{color:#a00000}.chroma .ge{font-style:italic}.chroma .gr{color:red}.chroma .gh{color:navy;font-weight:700}.chroma .gi{color:#00a000}.chroma .go{color:#888}.chroma .gp{color:#c65d09;font-weight:700}.chroma .gs{font-weight:700}.chroma .gu{color:purple;font-weight:700}.chroma .gt{color:#04d}.chroma .w{color:#bbb}</style></head><body><section id=nav><h1><a href=https://reorchestrate.com/>reorchestrate</a></h1><section class=section><div class=sub><p>&copy; Mike Seddon 2023 |
<a href=https://www.github.com/seddonm1 target=_blank><svg width="22" height="22" viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M896 128q209 0 385.5 103T1561 510.5 1664 896q0 251-146.5 451.5T1139 1625q-27 5-40-7t-13-30q0-3 .5-76.5t.5-134.5q0-97-52-142 57-6 102.5-18t94-39 81-66.5 53-105T1386 856q0-119-79-206 37-91-8-204-28-9-81 11t-92 44l-38 24q-93-26-192-26t-192 26q-16-11-42.5-27T578 459.5 493 446q-45 113-8 204-79 87-79 206 0 85 20.5 150t52.5 105 80.5 67 94 39 102.5 18q-39 36-49 103-21 10-45 15t-57 5-65.5-21.5T484 1274q-19-32-48.5-52t-49.5-24l-20-3q-21 0-29 4.5t-5 11.5 9 14 13 12l7 5q22 10 43.5 38t31.5 51l10 23q13 38 44 61.5t67 30 69.5 7 55.5-3.5l23-4q0 38 .5 88.5t.5 54.5q0 18-13 30t-40 7q-232-77-378.5-277.5T128 896q0-209 103-385.5T510.5 231 896 128zM419 1231q3-7-7-12-10-3-13 2-3 7 7 12 9 6 13-2zm31 34q7-5-2-16-10-9-16-3-7 5 2 16 10 10 16 3zm30 45q9-7 0-19-8-13-17-6-9 5 0 18t17 7zm42 42q8-8-4-19-12-12-20-3-9 8 4 19 12 12 20 3zm57 25q3-11-13-16-15-4-19 7t13 15q15 6 19-6zm63 5q0-13-17-11-16 0-16 11 0 13 17 11 16 0 16-11zm58-10q-2-11-18-9-16 3-14 15t18 8 14-14z"/></svg></a><a href=https://www.linkedin.com/in/mikeseddonau/ target=_blank><svg width="22" height="22" viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M365 1414h231V720H365v694zm246-908q-1-52-36-86t-93-34-94.5 34-36.5 86q0 51 35.5 85.5T479 626h1q59 0 95-34.5t36-85.5zm585 908h231v-398q0-154-73-233t-193-79q-136 0-209 117h2V720H723q3 66 0 694h231v-388q0-38 7-56 15-35 45-59.5t74-24.5q116 0 116 157v371zm468-998v960q0 119-84.5 203.5T1376 1664H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z"/></svg></a></p></div></section><ul></ul></section><section id=content><h1>Natural Language Processing with Apache Spark ML and Amazon Reviews (Part 1)</h1><div id=sub-header>5 December 2015</div><div class=entry-content><p>The most exciting feature of Apache Spark is it&rsquo;s &lsquo;generality&rsquo; meaning the ability to rapidly take some text data, transform it to a graph structure and perform some network analysis with <a href=https://spark.apache.org/graphx/>GraphX</a> take that dataset and apply some machine learning algorithms with <a href=https://spark.apache.org/mllib/>SparkML</a> and store it in memory and query it using <a href=https://spark.apache.org/sql/>SparkSQL</a> all within a single program of very little code.</p><p>In this post I wanted to write about the Spark ML framework and how easy and effective it is to do scalable machine learning by creating a pipeline which perform Natural Language Processing to assess whether a user&rsquo;s plain text review aligns with their rating (from 1 to 5).</p><p>The dataset I have chosen is an <a href=http://jmcauley.ucsd.edu/data/amazon/>Amazon Reviews dataset</a> which has been nicely curated by Julian McAuley from University of California, San Diego for use with their Machine Learning papers:</p><p><a href=http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf>Image-based recommendations on styles and substitutes</a>
J. McAuley, C. Targett, J. Shi, A. van den Hengel
SIGIR, 2015</p><p><a href=http://cseweb.ucsd.edu/~jmcauley/pdfs/kdd15.pdf>Inferring networks of substitutable and complementary products</a>
J. McAuley, R. Pandey, J. Leskovec
Knowledge Discovery and Data Mining, 2015</p><h3 id=1-know-your-data>1. Know your data</h3><p>The first thing to note is that the authors, who have extracted the data using Python, have generated in a non-standard JSON format which means that Spark&rsquo;s internal reader will not be able to read it. This tutorial was written with Spark 1.5.2 and from reading the <a href=https://issues.apache.org/jira/browse/SPARK-11745>Spark JIRA</a> it looks like 1.6.0 will allow the Spark JSON reader to deal with non-standard JSON. Otherwise it can be fixed quickly and easily using the simple Python script the authors provide on their site:</p><div class=highlight><pre class=chroma><code class=language-language-python data-lang=language-python>import json
import gzip

def parse(path):
  g = gzip.open(path, &#39;r&#39;)
  for l in g:
    yield json.dumps(eval(l))

f = open(&#34;output.strict&#34;, &#39;w&#39;)
for l in parse(&#34;reviews_Video_Games.json.gz&#34;):
  f.write(l + &#39;\n&#39;)</code></pre></div><p>I also took the opportunity to recompress the gzipped files to BZ2 so they would be splittable and could be more easily consumed downstream.</p><p>Let&rsquo;s look at the first row in the <code>reviews_Books.json.gz</code> file as we will be using this dataset for the rest of this post:</p><div class=highlight><pre class=chroma><code class=language-language-javascript data-lang=language-javascript>{
  &#34;summary&#34;: &#34;Show me the money!&#34;,
  &#34;overall&#34;: 4.0,
  &#34;reviewerID&#34;: &#34;AH2L9G3DQHHAJ&#34;,
  &#34;unixReviewTime&#34;: 1019865600,
  &#34;reviewText&#34;: &#34;Interesting Grisham tale of a lawyer that takes millions of dollars from his firm after faking his own death. Grisham usually is able to hook his readers early and ,in this case, doesn&#39;t play his hand to soon. The usually reliable Frank Mueller makes this story even an even better bet on Audiobook.&#34;,
  &#34;asin&#34;: &#34;0000000116&#34;,
  &#34;helpful&#34;: [5, 5],
  &#34;reviewTime&#34;: &#34;04 27, 2002&#34;,
  &#34;reviewerName&#34;: &#34;chris&#34;
}</code></pre></div><p>From this review we have learnt:</p><ul><li>There are two elements which we want for this tutorial. The <code>reviewText</code> which we need to parse and the <code>overall</code> which tells us what score the customer actually rated the product.</li><li>Based on my mental model I wouldn&rsquo;t be able to translate the <code>summary</code> &lsquo;Show me the money!&rsquo; text to any useful rating so it is unlikely to help in translating the dataset.</li><li>In this review I can determine there are 53 words which is actually lower than Amazon&rsquo;s recommended &lsquo;ideal length &hellip;[of] 75 to 500 words&rsquo;</li><li>If we are getting <code>reviewText</code> with as few as 53 words and potentially as much as 500+ word the model will have to be tuned to work out which words are important not just how frequently they appear.</li><li>As we know the <code>reviewerID</code> perhaps a model could incorporate some sort of adjustment factor for different reviewers that have skewed review scores compared to the population?</li><li>Perhaps the <code>helpful</code> ratings could be used to add additional weighting to reviews by indicating that people who thought the review was helpful had semi-validated that the text aligned with the score.</li></ul><p>In this exercise we are only going to use the <code>reviewText</code> and <code>overall</code> fields but potentially a much stronger model could be built with additional features.</p><p>The question is, can our model determine enough from the review text to accurately classify the text into one of the discrete <code>1.0</code>, <code>2.0</code>, <code>3.0</code>, <code>4.0</code>, <code>5.0</code> categories?</p><h3 id=2-load-the-files>2. Load the Files</h3><p>Copy <code>reviews_Books.json.bz2</code> to somewhere on your filesystem that can be accessed by your Apache Spark process. For development we are going to execute these Scala scripts using the <code>bin/spark-shell</code> utility:</p><div class=highlight><pre class=chroma><code class=language-language-bash data-lang=language-bash>bin/spark-shell -i filename.scala</code></pre></div><h4 id=2-1-setting-up>2.1 Setting up</h4><p>Let&rsquo;s start by loading libraries. You can see that this tutorial depends heavily on many different aspects of the <code>spark.ml</code> library particularly on the <code>feature</code> library which will do much of the data pre-processing for us:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>import</span> <span class=nn>org.apache.spark.ml.Pipeline</span>
<span class=k>import</span> <span class=nn>org.apache.spark.ml.PipelineModel</span>
<span class=k>import</span> <span class=nn>org.apache.spark.ml.classification.NaiveBayes</span>
<span class=k>import</span> <span class=nn>org.apache.spark.ml.classification.NaiveBayesModel</span>
<span class=k>import</span> <span class=nn>org.apache.spark.ml.feature.</span><span class=o>{</span><span class=nc>RegexTokenizer</span><span class=o>,</span><span class=nc>StopWordsRemover</span><span class=o>,</span><span class=nc>PorterStemmer</span><span class=o>,</span><span class=nc>NGram</span><span class=o>,,</span><span class=nc>HashingTF</span><span class=o>,</span><span class=nc>VectorAssembler</span><span class=o>,</span><span class=nc>StringIndexer</span><span class=o>,</span><span class=nc>IndexToString</span><span class=o>}</span>
<span class=k>import</span> <span class=nn>org.apache.spark.ml.tuning.</span><span class=o>{</span><span class=nc>ParamGridBuilder</span><span class=o>,</span> <span class=nc>CrossValidator</span><span class=o>}</span>
<span class=k>import</span> <span class=nn>org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator</span>
<span class=k>import</span> <span class=nn>org.apache.spark.mllib.util.MLUtils</span>

<span class=k>import</span> <span class=nn>org.apache.spark.sql.Row</span>
<span class=k>import</span> <span class=nn>org.apache.spark.sql.DataFrame</span>
<span class=k>import</span> <span class=nn>org.apache.spark.storage.StorageLevel</span></code></pre></div><h4 id=2-2-load-and-parse-the-data>2.2 Load and Parse the Data</h4><p>Now to load the data is extremely easy as we are using dataframes which will happily consume JSON files and create an SQL queryable dataset:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>reviews</span> <span class=k>=</span> <span class=s>&#34;data/reviews_Books.json.bz2&#34;</span>
<span class=k>var</span> <span class=n>reviewsDF</span> <span class=k>=</span> <span class=n>sqlContext</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>json</span><span class=o>(</span><span class=n>reviews</span><span class=o>)</span>
<span class=n>reviewsDF</span><span class=o>.</span><span class=n>registerTempTable</span><span class=o>(</span><span class=s>&#34;reviews&#34;</span><span class=o>)</span></code></pre></div><p>Three lines! Thanks Spark.</p><p>Now if we want to see the distribution of the 22507538 reviews we can run a very simple command. You can see that the data is massively skewed towards <code>5.0</code> star reviews (there are 14 <code>5.0</code> reviews to every 1 <code>2.0</code> review!</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>reviewsDF</span><span class=o>.</span><span class=n>groupBy</span><span class=o>(</span><span class=s>&#34;overall&#34;</span><span class=o>).</span><span class=n>count</span><span class=o>().</span><span class=n>orderBy</span><span class=o>(</span><span class=s>&#34;overall&#34;</span><span class=o>).</span><span class=n>show</span><span class=o>()</span>

<span class=o>+-------+--------+</span>
<span class=o>|</span><span class=n>overall</span><span class=o>|</span>   <span class=n>count</span><span class=o>|</span>
<span class=o>+-------+--------+</span>
<span class=o>|</span>    <span class=mf>1.0</span><span class=o>|</span> <span class=mi>1116877</span><span class=o>|</span>
<span class=o>|</span>    <span class=mf>2.0</span><span class=o>|</span>  <span class=mi>978575</span><span class=o>|</span>
<span class=o>|</span>    <span class=mf>3.0</span><span class=o>|</span> <span class=mi>1922423</span><span class=o>|</span>
<span class=o>|</span>    <span class=mf>4.0</span><span class=o>|</span> <span class=mi>4602643</span><span class=o>|</span>
<span class=o>|</span>    <span class=mf>5.0</span><span class=o>|</span><span class=mi>13887020</span><span class=o>|</span>
<span class=o>+-------+--------+</span></code></pre></div><p><img src=https://reorchestrate.com/img/2015/12/reviewsByOverall-1.png alt="Number of Reviews per overall"></p><h4 id=2-3-prepare-the-data>2.3 Prepare the data</h4><p>I want to prepare a subset of my <code>reviews</code> dataset which meets two goals:</p><ul><li>Extract the same number of reviews from each <code>1.0</code>, <code>2.0</code>, <code>3.0</code>, <code>4.0</code>, <code>5.0</code> category to train a generic model and ensure there are actually reviews for the model to learn from.</li><li>Separate <code>training</code> and <code>test</code> data into two different datasets so that when we test the model it can in no way be influenced by the <code>training</code> data.</li></ul><p>Why do I want the same number of reviews for each category?
Skewed data, or having more training examples for one category than another, can cause the decision boundary weights to be biased. This causes the classifier to unwittingly prefer one class over the other. There is very good information <a href=https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf>available here</a> about these biases.</p><p>In practice this means that the easy way of sampling data in Spark will, with this data set, produce a model which will appear to be performing classification very well but is in fact using a model which is highly skewed towards the <code>5.0</code> category:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>reviewsDF</span> <span class=k>=</span> <span class=n>reviewsDF</span><span class=o>.</span><span class=n>sample</span><span class=o>(</span><span class=n>withReplacement</span> <span class=k>=</span> <span class=kc>false</span><span class=o>,</span><span class=mf>0.2</span><span class=o>)</span></code></pre></div><p>To produce an equally weighted dataset we will use a SQL query because it makes it to easy to assign row numbers and choose an equal sized dataset for each category. I am ordering by <code>rand()</code> to take a quasi-random sample so if the model we build in this process is good I expect to get a very similar result every time we run this application on a different random dataset.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>var</span> <span class=n>reviewsDF</span> <span class=k>=</span> <span class=n>sqlContext</span><span class=o>.</span><span class=n>sql</span><span class=o>(</span>
<span class=s>&#34;&#34;&#34;
</span><span class=s>  SELECT text, label, rowNumber FROM (
</span><span class=s>    SELECT
</span><span class=s>       reviews.overall AS label
</span><span class=s>      ,reviews.reviewText AS text
</span><span class=s>      ,row_number() OVER (PARTITION BY overall ORDER BY rand()) AS rowNumber
</span><span class=s>    FROM reviews
</span><span class=s>  ) reviews
</span><span class=s>  WHERE rowNumber &lt;= 200000
</span><span class=s>  &#34;&#34;&#34;</span>
<span class=o>)</span>

<span class=n>reviewsDF</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_AND_DISK</span><span class=o>)</span>

<span class=n>reviewsDF</span><span class=o>.</span><span class=n>groupBy</span><span class=o>(</span><span class=s>&#34;label&#34;</span><span class=o>).</span><span class=n>count</span><span class=o>().</span><span class=n>orderBy</span><span class=o>(</span><span class=s>&#34;label&#34;</span><span class=o>).</span><span class=n>show</span><span class=o>()</span></code></pre></div><p>If we execute this code on a review dataset with at least 20000 reviews per category we would expect to see this result:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=o>+-----+-----+</span>
<span class=o>|</span><span class=n>label</span><span class=o>|</span><span class=n>count</span><span class=o>|</span>
<span class=o>+-----+-----+</span>
<span class=o>|</span>  <span class=mf>1.0</span><span class=o>|</span><span class=mi>20000</span><span class=o>|</span>
<span class=o>|</span>  <span class=mf>2.0</span><span class=o>|</span><span class=mi>20000</span><span class=o>|</span>
<span class=o>|</span>  <span class=mf>3.0</span><span class=o>|</span><span class=mi>20000</span><span class=o>|</span>
<span class=o>|</span>  <span class=mf>4.0</span><span class=o>|</span><span class=mi>20000</span><span class=o>|</span>
<span class=o>|</span>  <span class=mf>5.0</span><span class=o>|</span><span class=mi>20000</span><span class=o>|</span>
<span class=o>+-----+-----+</span></code></pre></div><p>This dataset now has 100000 records containing a random set of reviews in each category which all have a <code>rowNumber</code> field from 1 to 20000 which means we can now easily split out a <code>training</code> and <code>test</code> dataset which is guaranteed to have the correct number of rows and be as random as Spark SQL&rsquo;s <code>rand()</code> function is.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>training</span> <span class=k>=</span> <span class=n>reviewsDF</span><span class=o>.</span><span class=n>filter</span><span class=o>(</span><span class=n>reviewsDF</span><span class=o>(</span><span class=s>&#34;rowNumber&#34;</span><span class=o>)</span> <span class=o>&lt;=</span> <span class=mi>15000</span><span class=o>).</span><span class=n>select</span><span class=o>(</span><span class=s>&#34;text&#34;</span><span class=o>,</span><span class=s>&#34;label&#34;</span><span class=o>)</span>
<span class=k>val</span> <span class=n>test</span> <span class=k>=</span> <span class=n>reviewsDF</span><span class=o>.</span><span class=n>filter</span><span class=o>(</span><span class=n>reviewsDF</span><span class=o>(</span><span class=s>&#34;rowNumber&#34;</span><span class=o>)</span> <span class=o>&gt;</span> <span class=mi>15000</span><span class=o>).</span><span class=n>select</span><span class=o>(</span><span class=s>&#34;text&#34;</span><span class=o>,</span><span class=s>&#34;label&#34;</span><span class=o>)</span>

<span class=n>training</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_AND_DISK</span><span class=o>)</span>
<span class=n>test</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>spark</span><span class=o>.</span><span class=n>storage</span><span class=o>.</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_AND_DISK</span><span class=o>)</span>

<span class=k>val</span> <span class=n>numTraining</span> <span class=k>=</span> <span class=n>training</span><span class=o>.</span><span class=n>count</span><span class=o>()</span>
<span class=k>val</span> <span class=n>numTest</span> <span class=k>=</span> <span class=n>test</span><span class=o>.</span><span class=n>count</span><span class=o>()</span>

<span class=n>println</span><span class=o>(</span><span class=s>s&#34;numTraining = </span><span class=si>$numTraining</span><span class=s>, numTest = </span><span class=si>$numTest</span><span class=s>&#34;</span><span class=o>)</span></code></pre></div><h3 id=3-build-the-pipeline>3. Build the Pipeline</h3><p>What is a pipeline anyway? Basically it is a series of steps (called <code>stages</code>) that are applied to a dataset in series which will <code>transform</code> the data.</p><h4 id=3-1-a-visual-pipeline>3.1 A visual pipeline</h4><p>Let&rsquo;s have a look at the pipeline we are building first so we can understand what is happening. You can see that the pipeline consists of:</p><ul><li>the blue box represents the data set as it is the input to the model. For the training stage of the model building we will pass the input data set as <code>training</code> and when testing the model the data set will be <code>test</code>.</li><li>each of the yellow boxes represents a <code>transform</code> stage which will take an input column from the <code>training</code> dataset, perform some work and write out to an additional column in the dataset. A <code>transformer</code> will only operate on the current record.</li><li>the green boxes represent <code>estimator</code> steps (such as the <code>NaiveBayes()</code> machine learning algorithm) which will process the entire dataset and produce model which can then be used to <code>transform</code> a subsequent dataset. An <code>estimator</code> will operate over the entire dataset.</li><li>there are many more <code>transform</code> steps than anything else. This is because it is data and feature preparation which is the key work to prepare a machine learning model.</li></ul><p><img src=https://reorchestrate.com/img/2015/12/pipeline.png alt=pipeline></p><h4 id=3-2-now-let-s-code-it>3.2 Now let&rsquo;s code it</h4><h5 id=3-2-1-split-full-text-into-array-of-words>3.2.1 Split full text into array of words</h5><p>First we need to process the raw text and convert it into an array of words. I found the <code>RegexTokenizer</code> is more useful than the default space-based (&rsquo; &lsquo;) <code>tokenizer</code> as it will remove things like full stops and comma characters (<code>.</code>,<code>,</code>) and still deal with <code>doesn't</code> as a single token where \w would split it into <code>doesn</code> and <code>t</code>.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>regexTokenizer</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>RegexTokenizer</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setPattern</span><span class=o>(</span><span class=s>&#34;[a-zA-Z&#39;]+&#34;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setGaps</span><span class=o>(</span><span class=kc>false</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=s>&#34;text&#34;</span><span class=o>)</span>
<span class=o>}</span></code></pre></div><p>Output:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Array</span><span class=o>([</span><span class=kt>WrappedArray</span><span class=o>(</span><span class=kt>interesting</span>, <span class=kt>grisham</span>, <span class=kt>tale</span>, <span class=kt>of</span>, <span class=kt>a</span>, <span class=kt>lawyer</span>, <span class=kt>that</span>, <span class=kt>takes</span>, <span class=kt>millions</span>, <span class=kt>of</span>, <span class=kt>dollars</span>, <span class=kt>from</span>, <span class=kt>his</span>, <span class=kt>firm</span>, <span class=kt>after</span>, <span class=kt>faking</span>, <span class=kt>his</span>, <span class=kt>own</span>, <span class=kt>death</span>, <span class=kt>grisham</span>, <span class=kt>usually</span>, <span class=kt>is</span>, <span class=kt>able</span>, <span class=kt>to</span>, <span class=kt>hook</span>, <span class=kt>his</span>, <span class=kt>readers</span>, <span class=kt>early</span>, <span class=kt>and</span>, <span class=kt>in</span>, <span class=kt>this</span>, <span class=kt>case</span>, <span class=kt>doesn</span><span class=err>&#39;</span><span class=kt>t</span>, <span class=kt>play</span>, <span class=kt>his</span>, <span class=kt>hand</span>, <span class=kt>to</span>, <span class=kt>soon</span>, <span class=kt>the</span>, <span class=kt>usually</span>, <span class=kt>reliable</span>, <span class=kt>frank</span>, <span class=kt>mueller</span>, <span class=kt>makes</span>, <span class=kt>this</span>, <span class=kt>story</span>, <span class=kt>even</span>, <span class=kt>an</span>, <span class=kt>even</span>, <span class=kt>better</span>, <span class=kt>bet</span>, <span class=kt>on</span>, <span class=kt>audiobook</span><span class=o>)])</span></code></pre></div><h5 id=3-2-2-remove-stopwords>3.2.2 Remove Stopwords</h5><p>The next step in text processing often involves removing low-information words such as &lsquo;a&rsquo;, &lsquo;and&rsquo;, &lsquo;the&rsquo;. In the short review example above the word &lsquo;his&rsquo; appears four times. Intuitively we can understand that this is not a particularly useful word for classification as it is not an emotive or descriptive word and it is likely to appear in many reviews for each category so it won&rsquo;t help us split a positive from a negative review.</p><p>Because this function is so common it is built into the Spark core and you don&rsquo;t even need to specify the list of words. If you are interested you can see the list in the <a href=https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/StopWordsRemover.scala>source code</a> or if you are very motivated you can provide your own list (maybe for some industries this would be beneficial). A quick way to generate stopwords is to run wordcount over a list of books and the most frequent <em>n</em> words are likely low information words or stopwords.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>remover</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>StopWordsRemover</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=n>regexTokenizer</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>)</span>
<span class=o>}</span></code></pre></div><p>Output:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Array</span><span class=o>([</span><span class=kt>WrappedArray</span><span class=o>(</span><span class=kt>interesting</span>, <span class=kt>grisham</span>, <span class=kt>tale</span>, <span class=kt>lawyer</span>, <span class=kt>takes</span>, <span class=kt>millions</span>, <span class=kt>dollars</span>, <span class=kt>firm</span>, <span class=kt>faking</span>, <span class=kt>death</span>, <span class=kt>grisham</span>, <span class=kt>usually</span>, <span class=kt>able</span>, <span class=kt>hook</span>, <span class=kt>readers</span>, <span class=kt>early</span>, <span class=kt>case</span>, <span class=kt>doesn</span><span class=err>&#39;</span><span class=kt>t</span>, <span class=kt>play</span>, <span class=kt>hand</span>, <span class=kt>soon</span>, <span class=kt>usually</span>, <span class=kt>reliable</span>, <span class=kt>frank</span>, <span class=kt>mueller</span>, <span class=kt>makes</span>, <span class=kt>story</span>, <span class=kt>better</span>, <span class=kt>bet</span>, <span class=kt>audiobook</span><span class=o>)])</span></code></pre></div><h5 id=3-2-3-stemming>3.2.3 Stemming</h5><p>Stemming is a technique of reducing words to their <code>root</code> word or <code>stem</code>. For example, <code>stems</code>, <code>stemmer</code>, <code>stemming</code>, <code>stemmed</code> are all derivations of the root word <code>stem</code>. The reason we want to do this in a Natural Language Processor is to try to group words with the same meaning which should increase the frequency of that word occurring and hopefully it&rsquo;s information value. Keep in mind that the <code>stem</code> word does not have be the correct <a href=https://en.wikipedia.org/wiki/Root_(linguistics)>morphological root</a>, it just has to result in the same sequence of characters (in this algorithm <code>play</code> will stem to <code>plai</code> which doesn&rsquo;t matter as long as <code>played</code> and <code>playing</code> also produces <code>plai</code>).</p><p>In the John Grisham example, an obvious stemming opportunity would be the first word, <code>interesting</code> which we can stem to <code>interest</code>. Theoretically if we found the word <code>interested</code> in another review we would also stem that to <code>interest</code> hopefully resulting in a stronger model.</p><p>As of Spark 1.5.2 Stemming has not been introduced but I have taken the <a href=http://tartarus.org/martin/PorterStemmer/>Porter Stemmer</a> Algorithm implemented in Scala by the <a href=https://github.com/scalanlp/chalk>ScalaNLP</a> project and wrapped it as a Spark Transformer <a href=http://mike.seddon.ca/porter-stemming-in-apache-spark-ml/>available here</a>. Unfortunately, you are going to have to build Spark from source to use it.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>stemmer</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>PorterStemmer</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=n>remover</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>)</span>
<span class=o>}</span></code></pre></div><p>Output:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala> <span class=nc>Array</span><span class=o>([</span><span class=kt>WrappedArray</span><span class=o>(</span><span class=kt>interest</span>, <span class=kt>grisham</span>, <span class=kt>tale</span>, <span class=kt>lawyer</span>, <span class=kt>take</span>, <span class=kt>million</span>, <span class=kt>dollar</span>, <span class=kt>firm</span>, <span class=kt>fake</span>, <span class=kt>death</span>, <span class=kt>grisham</span>, <span class=kt>usual</span>, <span class=kt>abl</span>, <span class=kt>hook</span>, <span class=kt>reader</span>, <span class=kt>earli</span>, <span class=kt>case</span>, <span class=kt>doesn</span><span class=err>&#39;</span><span class=kt>t</span>, <span class=kt>plai</span>, <span class=kt>hand</span>, <span class=kt>soon</span>, <span class=kt>usual</span>, <span class=kt>reliabl</span>, <span class=kt>frank</span>, <span class=kt>mueller</span>, <span class=kt>make</span>, <span class=kt>stori</span>, <span class=kt>better</span>, <span class=kt>bet</span>, <span class=kt>audiobook</span><span class=o>)])</span></code></pre></div><h5 id=3-2-4-calculate-n-grams>3.2.4 Calculate <em>n</em>-grams</h5><p>The most basic sentiment analysers use a list of words which have been loosely classified into positive (<code>+1</code>) and negative (<code>-1</code>) for each word:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>good</span><span class=k>:</span> <span class=err>1</span>
<span class=kt>bad:</span> <span class=kt>-</span><span class=err>1</span>
<span class=kt>like:</span> <span class=err>1</span>
<span class=kt>hate:</span> <span class=kt>-</span><span class=err>1</span></code></pre></div><p>The algorithm searches for those words in sentences and replaces then with a <code>+1</code> or <code>-1</code> score or <code>0</code> if not found (the mapper) and then sums up the resulting score (the reducer). But what happens when we give a more interesting sentence such as:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>don</span>&#39;t <span class=n>like</span></code></pre></div><p>Depending on how basic the model is it is likely to come out as either positive (<code>+1</code>) due to the <code>like</code> (<code>+1</code>) keyword or maybe they also classify <code>don't</code> as a negative (<code>-1</code>) keyword which would sum to a <code>0</code> score. Either way, unless we consider both words as a pair we cannot interpret its true meaning and give the correct <code>-1</code> score. Consider what would happen with a less abbreviated version such as &lsquo;do not like&rsquo;.</p><p><em>n</em>-grams are a solution to this problem. A <em>n</em>-gram is a group of <em>n</em> words to be considered as a group. In the sentence above if we consider the &lsquo;don&rsquo;t like&rsquo; sentence as two words then a <em>2</em>-gram algorithm world consider them as one feature. Then if we had a simple model and we set:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=n>good</span><span class=k>:</span> <span class=err>1</span>
<span class=kt>bad:</span> <span class=kt>-</span><span class=err>1</span>
<span class=kt>don</span><span class=err>&#39;</span><span class=kt>t</span> <span class=kt>like:</span> <span class=kt>-</span><span class=err>1</span></code></pre></div><p>We can now calculate the correct sentiment score of <code>-1</code>.</p><p>In the workflow contained here we take the output of the <code>PorterStemmer()</code> stage and calculate <em>n</em>-grams of both 2 words and 3 words in length.</p><p><em>n</em>-gram(2):</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>ngram2</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>NGram</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=n>stemmer</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setN</span><span class=o>(</span><span class=mi>2</span><span class=o>)</span>
<span class=o>}</span></code></pre></div><p>Ouput:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Array</span><span class=o>([</span><span class=kt>WrappedArray</span><span class=o>(</span><span class=kt>interest</span> <span class=kt>grisham</span>, <span class=kt>grisham</span> <span class=kt>tale</span>, <span class=kt>tale</span> <span class=kt>lawyer</span>, <span class=kt>lawyer</span> <span class=kt>take</span>, <span class=kt>take</span> <span class=kt>million</span>, <span class=kt>million</span> <span class=kt>dollar</span>, <span class=kt>dollar</span> <span class=kt>firm</span>, <span class=kt>firm</span> <span class=kt>fake</span>, <span class=kt>fake</span> <span class=kt>death</span>, <span class=kt>death</span> <span class=kt>grisham</span>, <span class=kt>grisham</span> <span class=kt>usual</span>, <span class=kt>usual</span> <span class=kt>abl</span>, <span class=kt>abl</span> <span class=kt>hook</span>, <span class=kt>hook</span> <span class=kt>reader</span>, <span class=kt>reader</span> <span class=kt>earli</span>, <span class=kt>earli</span> <span class=kt>case</span>, <span class=kt>case</span> <span class=kt>doesn</span><span class=err>&#39;</span><span class=kt>t</span>, <span class=kt>doesn</span><span class=err>&#39;</span><span class=kt>t</span> <span class=kt>plai</span>, <span class=kt>plai</span> <span class=kt>hand</span>, <span class=kt>hand</span> <span class=kt>soon</span>, <span class=kt>soon</span> <span class=kt>usual</span>, <span class=kt>usual</span> <span class=kt>reliabl</span>, <span class=kt>reliabl</span> <span class=kt>frank</span>, <span class=kt>frank</span> <span class=kt>mueller</span>, <span class=kt>mueller</span> <span class=kt>make</span>, <span class=kt>make</span> <span class=kt>stori</span>, <span class=kt>stori</span> <span class=kt>better</span>, <span class=kt>better</span> <span class=kt>bet</span>, <span class=kt>bet</span> <span class=kt>audiobook</span><span class=o>)])</span></code></pre></div><p><em>n</em>-gram(3):</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>ngram3</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>NGram</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=n>stemmer</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setN</span><span class=o>(</span><span class=mi>3</span><span class=o>)</span>
<span class=o>}</span></code></pre></div><p>Output:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Array</span><span class=o>([</span><span class=kt>WrappedArray</span><span class=o>(</span><span class=kt>interest</span> <span class=kt>grisham</span> <span class=kt>tale</span>, <span class=kt>grisham</span> <span class=kt>tale</span> <span class=kt>lawyer</span>, <span class=kt>tale</span> <span class=kt>lawyer</span> <span class=kt>take</span>, <span class=kt>lawyer</span> <span class=kt>take</span> <span class=kt>million</span>, <span class=kt>take</span> <span class=kt>million</span> <span class=kt>dollar</span>, <span class=kt>million</span> <span class=kt>dollar</span> <span class=kt>firm</span>, <span class=kt>dollar</span> <span class=kt>firm</span> <span class=kt>fake</span>, <span class=kt>firm</span> <span class=kt>fake</span> <span class=kt>death</span>, <span class=kt>fake</span> <span class=kt>death</span> <span class=kt>grisham</span>, <span class=kt>death</span> <span class=kt>grisham</span> <span class=kt>usual</span>, <span class=kt>grisham</span> <span class=kt>usual</span> <span class=kt>abl</span>, <span class=kt>usual</span> <span class=kt>abl</span> <span class=kt>hook</span>, <span class=kt>abl</span> <span class=kt>hook</span> <span class=kt>reader</span>, <span class=kt>hook</span> <span class=kt>reader</span> <span class=kt>earli</span>, <span class=kt>reader</span> <span class=kt>earli</span> <span class=kt>case</span>, <span class=kt>earli</span> <span class=kt>case</span> <span class=kt>doesn</span><span class=err>&#39;</span><span class=kt>t</span>, <span class=kt>case</span> <span class=kt>doesn</span><span class=err>&#39;</span><span class=kt>t</span> <span class=kt>plai</span>, <span class=kt>doesn</span><span class=err>&#39;</span><span class=kt>t</span> <span class=kt>plai</span> <span class=kt>hand</span>, <span class=kt>plai</span> <span class=kt>hand</span> <span class=kt>soon</span>, <span class=kt>hand</span> <span class=kt>soon</span> <span class=kt>usual</span>, <span class=kt>soon</span> <span class=kt>usual</span> <span class=kt>reliabl</span>, <span class=kt>usual</span> <span class=kt>reliabl</span> <span class=kt>frank</span>, <span class=kt>reliabl</span> <span class=kt>frank</span> <span class=kt>mueller</span>, <span class=kt>frank</span> <span class=kt>mueller</span> <span class=kt>make</span>, <span class=kt>mueller</span> <span class=kt>make</span> <span class=kt>stori</span>, <span class=kt>make</span> <span class=kt>stori</span> <span class=kt>better</span>, <span class=kt>stori</span> <span class=kt>better</span> <span class=kt>bet</span>, <span class=kt>better</span> <span class=kt>bet</span> <span class=kt>audiobook</span><span class=o>)])</span></code></pre></div><h5 id=3-2-5-term-frequency-hashing>3.2.5 Term Frequency Hashing</h5><p>At it&rsquo;s core, a machine learning classifier is the process of determining the probability of a certain <em>feature</em> appearing in a certain <em>class</em> thereby allowing us to predict a likely classification based on an input containing one or more of those <em>features</em>.</p><p>If we had infinite compute then it would make sense to treat each distinct <code>word</code> or <code>n-gram</code> as a feature and calculate each one&rsquo;s unique probability giving the most accurate model. The Oxford Dictionary has around 171,476 words in current use, and 47,156 obsolete words which is English only before all the spelling mistakes and abbreviations that are commonly found in Amazon reviews (go and try to read some smartphone game reviews if you want to know grammatical pain) so the problem space gets very large. Even having scalable architecture such as Spark does not forgive inefficient code.</p><p>Term Frequency Hashing helps as it is a dimensionality reduction technique to reduce the number of features by applying a hashing function. A key argument of the hashing function is how many discrete values or features will be produced and have their frequencies counted.</p><p>For example <em>(and don&rsquo;t do this)</em> if we were to run a the Term Frequency Hashing transformer <code>hashingTF()</code> over the stemmed dataset with a target of two features by calling <code>.setNumFeatures(2)</code> we would get this result:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Array</span><span class=o>([</span><span class=kt>WrappedArray</span><span class=o>(</span><span class=kt>interest</span>, <span class=kt>grisham</span>, <span class=kt>tale</span>, <span class=kt>lawyer</span>, <span class=kt>take</span>, <span class=kt>million</span>, <span class=kt>dollar</span>, <span class=kt>firm</span>, <span class=kt>fake</span>, <span class=kt>death</span>, <span class=kt>grisham</span>, <span class=kt>usual</span>, <span class=kt>abl</span>, <span class=kt>hook</span>, <span class=kt>reader</span>, <span class=kt>earli</span>, <span class=kt>case</span>, <span class=kt>doesn</span><span class=err>&#39;</span><span class=kt>t</span>, <span class=kt>plai</span>, <span class=kt>hand</span>, <span class=kt>soon</span>, <span class=kt>usual</span>, <span class=kt>reliabl</span>, <span class=kt>frank</span>, <span class=kt>mueller</span>, <span class=kt>make</span>, <span class=kt>stori</span>, <span class=kt>better</span>, <span class=kt>bet</span>, <span class=kt>audiobook</span><span class=o>)])</span>

<span class=nc>Array</span><span class=o>([(</span><span class=err>2</span>,<span class=o>[</span><span class=err>0</span>,<span class=err>1</span><span class=o>]</span>,<span class=o>[</span><span class=err>16</span><span class=kt>.</span><span class=err>0</span>,<span class=err>14</span><span class=kt>.</span><span class=err>0</span><span class=o>])])</span></code></pre></div><p>This means that the <code>hashingTF()</code> transformer has correctly produced two values (<code>0</code>,<code>1</code>) which have a count of <code>16</code> and <code>14</code> values respectively. The issue is that we have lost so much information by specifying so few features that the output is not useful for model building (we have deliberately created hash collisions due to such a small number of hash values).</p><p>If we run the same <code>hashingTF()</code> with 1000 features (<code>.setNumFeatures(1000)</code>) then we will see an array produced like this where the first line represents the hashed index value (which must be between 0 and 999) and the frequency. If you read the stemmed dataset you can see two words <code>grisham</code> and <code>usually</code> which appear twice and must have hashed to index of <code>583</code> or <code>589</code> in the hashing table (1000) as those are the only two which have frequency of <code>2.0</code>. We don&rsquo;t actually need to know which value is which as long as we use the same hashing function when we want to execute this model. The reason the dataset is so sparse is that we have only processed one row of data which has not produced 1000 distinct hash values from the words contained in that row.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Array</span><span class=o>([(</span><span class=err>1000</span>,<span class=o>[</span><span class=err>15</span>,<span class=err>50</span>,<span class=err>128</span>,<span class=err>170</span>,<span class=err>187</span>,<span class=err>192</span>,<span class=err>204</span>,<span class=err>207</span>,<span class=err>210</span>,<span class=err>230</span>,<span class=err>236</span>,<span class=err>266</span>,<span class=err>354</span>,<span class=err>371</span>,<span class=err>422</span>,<span class=err>425</span>,<span class=err>483</span>,<span class=err>508</span>,<span class=err>519</span>,<span class=err>575</span>,<span class=err>583</span>,<span class=err>589</span>,<span class=err>600</span>,<span class=err>624</span>,<span class=err>698</span>,<span class=err>908</span>,<span class=err>991</span>,<span class=err>997</span><span class=o>]</span>,<span class=o>[</span><span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span><span class=o>])])</span></code></pre></div><p>I want to apply the <code>hashingTF()</code> transformer against all my three working datasets but not specifying the number of features yet:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>removerHashingTF</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>HashingTF</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=n>stemmer</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>)</span>
<span class=o>}</span>
<span class=k>val</span> <span class=n>ngram2HashingTF</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>HashingTF</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=n>ngram2</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>)</span>
<span class=o>}</span>
<span class=k>val</span> <span class=n>ngram3HashingTF</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>HashingTF</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=n>ngram3</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>)</span>
<span class=o>}</span></code></pre></div><h5 id=3-2-6-vector-assembler>3.2.6 Vector Assembler</h5><p>After executing Term-Frequency Mapping across our three datasets we will end up with three vector columns containing hash indices and their term frequencies. <code>VectorAssembler()</code> will combine these columns into one so that it can be processed as the input column for the <code>NaiveBayes</code> algorithm we are going to run.</p><p>You can see from the Input/Output below that it will basically do a <code>union</code> of the datasets but will adjust the keys by the length of the hash function (<code>1000</code>) to avoid collisions. It is easy to see how this works by looking at the output at keys <code>991</code> and <code>1109</code>. Because we know the <code>HashingTF()</code> function was set to product <code>1000</code> features the resulting indices can only be between <code>1</code> and <code>999</code>. This means anything after <code>999</code> must be in the second set. As the value is <code>1109</code> and the first value in the second array is <code>109</code> you can see the transformer has added the length of the array (<code>1000</code>) to the key (<code>109</code>) and unioned the datasets.</p><p>Vector Assembler:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>assembler</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>VectorAssembler</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCols</span><span class=o>(</span><span class=nc>Array</span><span class=o>(</span><span class=n>removerHashingTF</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>,</span> <span class=n>ngram2HashingTF</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>,</span> <span class=n>ngram3HashingTF</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>))</span>
<span class=o>}</span></code></pre></div><p>Input:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Array</span><span class=o>([(</span><span class=err>1000</span>,<span class=o>[</span><span class=err>50</span>,<span class=err>170</span>,<span class=err>187</span>,<span class=err>192</span>,<span class=err>204</span>,<span class=err>230</span>,<span class=err>266</span>,<span class=err>317</span>,<span class=err>355</span>,<span class=err>363</span>,<span class=err>391</span>,<span class=err>422</span>,<span class=err>425</span>,<span class=err>428</span>,<span class=err>467</span>,<span class=err>474</span>,<span class=err>483</span>,<span class=err>492</span>,<span class=err>519</span>,<span class=err>575</span>,<span class=err>600</span>,<span class=err>602</span>,<span class=err>611</span>,<span class=err>854</span>,<span class=err>908</span>,<span class=err>972</span>,<span class=err>981</span>,<span class=err>991</span><span class=o>]</span>,<span class=o>[</span><span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span><span class=o>])])</span>
<span class=nc>Array</span><span class=o>([(</span><span class=err>1000</span>,<span class=o>[</span><span class=err>109</span>,<span class=err>141</span>,<span class=err>197</span>,<span class=err>204</span>,<span class=err>275</span>,<span class=err>294</span>,<span class=err>321</span>,<span class=err>323</span>,<span class=err>339</span>,<span class=err>417</span>,<span class=err>437</span>,<span class=err>585</span>,<span class=err>588</span>,<span class=err>618</span>,<span class=err>632</span>,<span class=err>671</span>,<span class=err>682</span>,<span class=err>701</span>,<span class=err>784</span>,<span class=err>818</span>,<span class=err>859</span>,<span class=err>864</span>,<span class=err>867</span>,<span class=err>927</span>,<span class=err>936</span>,<span class=err>940</span>,<span class=err>947</span><span class=o>]</span>,<span class=o>[</span><span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span><span class=o>])])</span>
<span class=nc>Array</span><span class=o>([(</span><span class=err>1000</span>,<span class=o>[</span><span class=err>10</span>,<span class=err>87</span>,<span class=err>92</span>,<span class=err>101</span>,<span class=err>178</span>,<span class=err>180</span>,<span class=err>278</span>,<span class=err>312</span>,<span class=err>329</span>,<span class=err>368</span>,<span class=err>384</span>,<span class=err>431</span>,<span class=err>473</span>,<span class=err>522</span>,<span class=err>523</span>,<span class=err>589</span>,<span class=err>677</span>,<span class=err>695</span>,<span class=err>718</span>,<span class=err>729</span>,<span class=err>742</span>,<span class=err>811</span>,<span class=err>877</span>,<span class=err>879</span>,<span class=err>936</span>,<span class=err>957</span>,<span class=err>971</span><span class=o>]</span>,<span class=o>[</span><span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span><span class=o>])])</span></code></pre></div><p>Output:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=nc>Array</span><span class=o>([(</span><span class=err>3000</span>,<span class=o>[</span><span class=err>50</span>,<span class=err>170</span>,<span class=err>187</span>,<span class=err>192</span>,<span class=err>204</span>,<span class=err>230</span>,<span class=err>266</span>,<span class=err>317</span>,<span class=err>355</span>,<span class=err>363</span>,<span class=err>391</span>,<span class=err>422</span>,<span class=err>425</span>,<span class=err>428</span>,<span class=err>467</span>,<span class=err>474</span>,<span class=err>483</span>,<span class=err>492</span>,<span class=err>519</span>,<span class=err>575</span>,<span class=err>600</span>,<span class=err>602</span>,<span class=err>611</span>,<span class=err>854</span>,<span class=err>908</span>,<span class=err>972</span>,<span class=err>981</span>,<span class=err>991</span>,<span class=err>1109</span>,<span class=err>1141</span>,<span class=err>1197</span>,<span class=err>1204</span>,<span class=err>1275</span>,<span class=err>1294</span>,<span class=err>1321</span>,<span class=err>1323</span>,<span class=err>1339</span>,<span class=err>1417</span>,<span class=err>1437</span>,<span class=err>1585</span>,<span class=err>1588</span>,<span class=err>1618</span>,<span class=err>1632</span>,<span class=err>1671</span>,<span class=err>1682</span>,<span class=err>1701</span>,<span class=err>1784</span>,<span class=err>1818</span>,<span class=err>1859</span>,<span class=err>1864</span>,<span class=err>1867</span>,<span class=err>1927</span>,<span class=err>1936</span>,<span class=err>1940</span>,<span class=err>1947</span>,<span class=err>2010</span>,<span class=err>2087</span>,<span class=err>2092</span>,<span class=err>2101</span>,<span class=err>2178</span>,<span class=err>2180</span>,<span class=err>2278</span>,<span class=err>2312</span>,<span class=err>2329</span>,<span class=err>2368</span>,<span class=err>2384</span>,<span class=err>2431</span>,<span class=err>2473</span>,<span class=err>2522</span>,<span class=err>2523</span>,<span class=err>2589</span>,<span class=err>2677</span>,<span class=err>2695</span>,<span class=err>2718</span>,<span class=err>2729</span>,<span class=err>2742</span>,<span class=err>2811</span>,<span class=err>2877</span>,<span class=err>2879</span>,<span class=err>2936</span>,<span class=err>2957</span>,<span class=err>2971</span><span class=o>]</span>,<span class=o>[</span><span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>2</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span>,<span class=err>1</span><span class=kt>.</span><span class=err>0</span><span class=o>])])</span></code></pre></div><h4 id=3-3-running-the-model>3.3 Running the Model</h4><p>At this point we have extracted all our <em>features</em> and at first I happily loaded them into the <code>NaiveBayes()</code> estimator, used it to transform my <code>test</code> dataset which produced nice <code>0.0</code>,<code>1.0</code>,<code>2.0</code>,<code>3.0</code>,<code>4.0</code> values which I then compared against my input labels which also look like <code>1.0</code>,<code>2.0</code>,<code>3.0</code>,<code>4.0</code>,<code>5.0</code> &hellip; with absolutely horrible results (~10%).</p><p><img src=https://reorchestrate.com/img/2015/12/ItsATrap.gif alt="It's a Trap"></p><p>I was unlucky enough to have numeric labels as if I had strings such as (<code>very good</code>, <code>good</code>, &hellip;) it have highlighted the issue straight away: the <code>NaiveBayes()</code> transformer will produce indexed values as outputs. Using <code>StringIndexer()</code> means you should always ensure that predictions produced by the model can be mapped back to the original labels.</p><p>Fortunately it is very easy to convert to indexed values with <code>StringIndexer()</code> and back to their original labels with <code>IndexToString()</code>.</p><h5 id=3-3-1-label-indexer>3.3.1 Label Indexer</h5><p>The <code>StringIndexer()</code> is actually an <em>estimator</em> which basically converts the distinct labels in the input dataset to index values e.g. (<code>0.0</code>,<code>1.0</code>,<code>2.0</code>,&hellip;<code>n</code>). The only trick it does is it will assign the indices in order of the frequency which the label appears in the dataset passed in the <code>fit()</code> parameter (and it has to assess the entire dataset to do so).</p><p>Label Indexer:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>labelIndexer</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>StringIndexer</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=s>&#34;label&#34;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setOutputCol</span><span class=o>(</span><span class=s>&#34;indexedLabel&#34;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>fit</span><span class=o>(</span><span class=n>reviewsDF</span><span class=o>)</span>
<span class=o>}</span></code></pre></div><p>Output:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=c1>// label, StringIndexer(label)
</span><span class=c1></span><span class=nc>Array</span><span class=o>([</span><span class=err>4</span><span class=kt>.</span><span class=err>0</span>,<span class=err>0</span><span class=kt>.</span><span class=err>0</span><span class=o>])</span></code></pre></div><p>So when we have only one value we get label <code>4.0</code> = <code>0.0</code> but this automatic assigning of indexed values may trick you up unless you use the next transformer, <code>IndexToString()</code>.</p><h5 id=3-3-2-label-converter>3.3.2 Label Converter</h5><p>When the <code>StringIndexer()</code> it stores the mapping table allowing the <code>IndexToString()</code> transformer to convert indices back to the original label.</p><p>Label Converter:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>labelConverter</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>IndexToString</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=s>&#34;prediction&#34;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setOutputCol</span><span class=o>(</span><span class=s>&#34;predictedLabel&#34;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setLabels</span><span class=o>(</span><span class=n>labelIndexer</span><span class=o>.</span><span class=n>labels</span><span class=o>)</span>
<span class=o>}</span></code></pre></div><p>Output:</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=c1>// label, StringIndexer(label), IndexToString(StringIndexer(label))
</span><span class=c1></span><span class=nc>Array</span><span class=o>([</span><span class=err>4</span><span class=kt>.</span><span class=err>0</span>,<span class=err>0</span><span class=kt>.</span><span class=err>0</span>,<span class=err>4</span><span class=kt>.</span><span class=err>0</span><span class=o>])</span></code></pre></div><p>Basically we just need to call that after our <code>NaiveBayes()</code> transformer to ensure our labels are correct.</p><h5 id=3-3-3-naive-bayes>3.3.3 Naive Bayes</h5><p>I will leave WikiPedia to explain why <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive Bayes</a> is a popular algorithm for Natural Language Processing tasks like spam filtering or sentiment analysis which is what we are doing here. Essentially it will calculate the probability of a certain word or phrase appearing for a given label and use that model to predict labels from new input data.</p><div class=highlight><pre class=chroma><code class=language-scala data-lang=scala><span class=k>val</span> <span class=n>nb</span> <span class=k>=</span> <span class=o>{</span> <span class=k>new</span> <span class=nc>NaiveBayes</span><span class=o>()</span>
  <span class=o>.</span><span class=n>setLabelCol</span><span class=o>(</span><span class=n>labelIndexer</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setFeaturesCol</span><span class=o>(</span><span class=n>assembler</span><span class=o>.</span><span class=n>getOutputCol</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setPredictionCol</span><span class=o>(</span><span class=s>&#34;prediction&#34;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>setModelType</span><span class=o>(</span><span class=s>&#34;multinomial&#34;</span><span class=o>)</span>
<span class=o>}</span></code></pre></div></div><div id=source>If you find an error please raise a <a class="basic-alignment left" href=https://github.com/seddonm1/seddonm1.github.io/blob/main/content/posts/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1.md>pull request</a>.</div><div id=links><a class="basic-alignment left" href=https://reorchestrate.com/posts/performance-tuning-spark-pagerank/>&laquo; Performance Tuning Spark WikiPedia PageRank</a>
<a class="basic-alignment left" href=https://reorchestrate.com/posts/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-2/>Natural Language Processing with Apache Spark ML and Amazon Reviews (Part 2) &raquo;</a></div></section></body></html>